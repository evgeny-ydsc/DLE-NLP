{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ce0391",
   "metadata": {},
   "source": [
    "# Проект: pretraining LLM и posttraining LLM\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "- **Pretraining**\n",
    "    - Претрейн — самый ресурсоёмкий этап обучения LLM. Чтобы полноценно обучить даже небольшую модель (менее 1B), понадобится более 10к GPU-часов на A100. Чтобы не тратить недели на обучение, но отработать ключевые приёмы, в проекте вы выполните упрощённую задачу. \n",
    "    - При полноценном претрейне модель учится обобщать знания из данных, на которых происходило обучение, чтобы потом извлекать эти знания по текстовым запросам уже после обучения. Упростим задачу — научим модель только структуре языка. \n",
    "    - Сосредоточимся на одном узком домене — **текстах произведений русской литературы** — и **обучим модель продолжать фразы из этого домена разумным текстом**. \n",
    "- **Posttraining**\n",
    "    - Для SFT-этапа можно использовать значительно меньше данных, поэтому возьмём модель крупнее. Рассмотрим базовую модель **Qwen2.5-0.5B**, с которой вы встречались в уроках. **Обучите её генерировать ответы на инструктивные русскоязычные вопросы**.\n",
    "\n",
    "## Критерии успеха\n",
    "- **Pretraining** - **чтобы оценить качество, используйте промпты**:\n",
    "    - \"Все мысли, которые имеют огромные последствия\",\n",
    "    - \"Сила войска зависит от его духа\",\n",
    "    - \"Мысль о том, что он принес страдания\",\n",
    "    - \"Человек сознает себя свободным\",\n",
    "    - \"Что бы ни случилось, я всегда буду\",\n",
    "    - \"Любовь мешает смерти\",\n",
    "    - \"Нет, жизнь не кончена\",\n",
    "    - \"Всякая мысль, даже самая простая\",\n",
    "    - \"Война не любезность, а самое гадкое дело\",\n",
    "    - \"Чтобы жить честно\"\n",
    "- **Posttraining** - Факты могут быть ошибочными, но **язык ответа должен быть русским и должна сохраняться структура ответа**. Качество данных **должно быть сопоставимо с таким вариантом**:\n",
    "    - Model Input 1:\n",
    "    сколько планет в нашей солнечной системе?\n",
    "        - Model Output 1:\n",
    "    Согласно последним исследованиям, Солнце имеет 8 планеты. Это дает планетам, которые соответствуют разным критериям, таким как диета, местоположение, плотность, климат и другие.\n",
    "    assistant\n",
    "    Общее размер нашей Солнечной системы составляет примерно 9,9 миллиарда километров, а в результате на каждом из них есть планета,\n",
    "    - Model Input 2:\n",
    "    расскажи стих\n",
    "        - Model Output 2:\n",
    "    Вот сладкий сладкий вкус, который ты получаешь на себе\n",
    "    Когда я говорю, что я люблю тебя\n",
    "    Мое сердце, у меня есть все, что я хочу, чтобы сделать\n",
    "    У меня есть все, что мне нужно, чтобы быть хорошим\n",
    "    Так давай пойдем в ресторан\n",
    "    assistant\n",
    "    Пока мы уседим, что мы можем\n",
    "    И в этот момент мы возвращаемся к при\n",
    "    - Model Input 3:\n",
    "    когда собирать крыжовник?\n",
    "        - Model Output 3:\n",
    "    Когда собираешь крыжовник, важно не беспокоиться о том, что другие могут его хвастаться. Вместо этого проверяйте свои способности и готовность. Вы можете изучить свой талант и стремления, прежде чем принимать решение, и признавать, что у вас есть свои сильные стороны, а также слабые места. Не снимайте крыжовник в обществе, и обязательно помните о\n",
    "    - Model Input 4:\n",
    "    Как быстро выучить новый язык?\n",
    "        - Model Output 4:\n",
    "    Скорость усвоения нового языка может сильно различаться в зависимости от нескольких факторов, таких как скорость, с которой вы пакетируете информацию через ее и ее способность учиться в различных условиях. Тем не менее, вот некоторые общие рекомендации: 1. Делайте сценарии. Обычно это может занять от 2 до 4 лет, чтобы понять основные концепции. Вы можете начать с"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343bf23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\" style=\"background-color:#D9EEE1;color:black;\">\n",
    "\n",
    "## Описание данных\n",
    "\n",
    "- Pretrain\n",
    "    - https://github.com/JoannaBy/RussianNovels/tree/master/corpus \n",
    "- Posttrain\n",
    "    - русскоязычный инструктивный датасет [d0rj/alpaca-cleaned-ru](https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru) в диалоговом формате"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905d42a",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fd01d",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18dc8e98",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install -U transformers==4.48.3 accelerate==0.33.0 trl==0.10.1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc2623a4",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install GitPython -q"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fb9202a3",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install -U \"transformers>=4.48.0\" \"accelerate>=0.30.0\" \"trl>=0.9.6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb23077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Papa\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from git import Repo\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, pre_tokenizers, trainers\n",
    "from transformers import (\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl\n",
    ")\n",
    "from tokenizers.models import BPE\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import accelerate\n",
    "from trl import SFTTrainer, SFTConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b78884",
   "metadata": {},
   "source": [
    "### Установка главных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e48b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "JUST_1ST_DEBUG = True  # True = временно для отладки на малом дaтасете перед GPU\n",
    "\n",
    "PRETRAIN_DATASET =  \"pretrain-dataset\"\n",
    "DICT_SIZE = 3000\n",
    "SPECIAL_TOKEN = \"<|endoftext|>\"\n",
    "CONTEXT_LENGTH = 512\n",
    "\n",
    "SFT_DATASET = \"d0rj/alpaca-cleaned-ru\"\n",
    "SFT_MODEL = \"Qwen/Qwen3-1.7B\"\n",
    "FINETUNED_MODEL_PATH = \"full_sft.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f98b8",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f277c58",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "\n",
    "Скачайте данные из репозитория и упакуйте их в один датасет. Вам понадобятся все произведения из репозитория.\n",
    "\n",
    "Проведите препроцессинг данных:\n",
    "- Очистите их от дубликатов.\n",
    "- Очистите от предложений с буквами не из кириллицы.\n",
    "- Обработайте повторяющуюся пунктуацию и т. д.\n",
    "- Разбейте на чанки поменьше, чтобы можно было добавить <bos> и <eos> токены в соответствии с обучаемой длиной контекста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aff8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(PRETRAIN_DATASET):\n",
    "    Repo.clone_from(\"https://github.com/JoannaBy/RussianNovels.git\", PRETRAIN_DATASET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "929d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file_path in Path(Path(PRETRAIN_DATASET, \"corpus\")).glob(\"*.txt\"):\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            texts.append(line.strip())\n",
    "\n",
    "# удалим строки с недопустимыми символами\n",
    "allowed_pattern = re.compile(r\"^[А-Яа-яЁё0-9.,!?;:\\-()\\\"'\\s]+$\")\n",
    "texts = [t for t in texts if allowed_pattern.fullmatch(t)]\n",
    "\n",
    "# Заменяем повторяющиеся знаки пунктуации на один\n",
    "texts = [re.sub(r'([.,!?;:\\-()])\\1+', r'\\1', t) for t in texts]\n",
    "\n",
    "#удалим дубликаты\n",
    "texts = pd.DataFrame({'t': texts}).drop_duplicates()['t'].tolist()\n",
    "\n",
    "#перенесём строки, если они длиннее 300 слов\n",
    "max_words = 300 # из рассчёта, что ~1.5 токена на слово, а токенов у нас в контексте 512\n",
    "result = []\n",
    "for text in texts:\n",
    "    words = text.split()\n",
    "    if len(words) > max_words:\n",
    "        parts = [' '.join(words[i:i + max_words]) \\\n",
    "                    for i in range(0, len(words), max_words)]\n",
    "        result.extend(parts)\n",
    "    else:\n",
    "        result.append(text)\n",
    "texts = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f3d605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- Тяжкое, тяжкое время, что говорить, - пробормотал он, - но унывать-то'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_TEXT = texts[100]\n",
    "TEST_TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77515878",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "Создайте и обучите собственный токенизатор на полученных данных. Размер словаря выберите небольшим: при обучении только на рассмотренных текстах — около 3к токенов. В рассматриваемых данных язык намного менее разнообразен, чем в совокупных данных, поэтому крупные токенизаторы от реальных LLM могут не подойти. \n",
    "При создании токенизатора можете ориентироваться на [материал huggingface.co](https://huggingface.co/learn/llm-course/ru/chapter6/8). Рекомендуем использовать BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea73b56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', (0, 1)),\n",
       " ('ĠÐ¢ÑıÐ¶ÐºÐ¾Ðµ', (1, 8)),\n",
       " (',', (8, 9)),\n",
       " ('ĠÑĤÑıÐ¶ÐºÐ¾Ðµ', (9, 16)),\n",
       " ('ĠÐ²ÑĢÐµÐ¼Ñı', (16, 22)),\n",
       " (',', (22, 23)),\n",
       " ('ĠÑĩÑĤÐ¾', (23, 27)),\n",
       " ('ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸ÑĤÑĮ', (27, 36)),\n",
       " (',', (36, 37)),\n",
       " ('Ġ-', (37, 39)),\n",
       " ('ĠÐ¿ÑĢÐ¾Ð±Ð¾ÑĢÐ¼Ð¾ÑĤÐ°Ð»', (39, 51)),\n",
       " ('ĠÐ¾Ð½', (51, 54)),\n",
       " (',', (54, 55)),\n",
       " ('Ġ-', (55, 57)),\n",
       " ('ĠÐ½Ð¾', (57, 60)),\n",
       " ('ĠÑĥÐ½ÑĭÐ²Ð°ÑĤÑĮ', (60, 68)),\n",
       " ('-', (68, 69)),\n",
       " ('ÑĤÐ¾', (69, 71))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(TEST_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ed49b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(texts):\n",
    "    for i in range(0, len(texts), 1000):\n",
    "        batch = [texts[j] for j in range(i, min(i + 1000, len(texts)))]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e324527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 6s\n",
      "Wall time: 25.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = trainers.BpeTrainer(vocab_size=DICT_SIZE, special_tokens=[SPECIAL_TOKEN])\n",
    "tokenizer.train_from_iterator(get_training_corpus(texts), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c681be0",
   "metadata": {},
   "source": [
    "Посмотрим на токенизатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "253297be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3000\n",
      "Sample vocab: ['ĠÐŁÑĢ', 'ĠÐ¿Ð¾Ð¹', 'ĠÐ¾Ð±ÐµÑī', 'ĠÐ¼ÐµÑĤ', 'ĠÑĢÐµÐ²', 'ĠÑĤÐ°Ð¼', 'ĠÑĤÐ¾Ð»ÑĮÐºÐ¾', 'ĠÐ¸Ð½', '!\"', 'ĠÐ¾Ð±ÑĢÐ°Ñī', 'Ð½Ð¸ÐµÐ¼', 'ÐºÐ°Ð¼', 'ĠÑĩÐ°ÑģÑĤÐ¾', 'ÐµÐ½Ð½ÑĥÑİ', 'Ð°Ð¿', 'ĠÐ¾Ð±Ðµ', 'ĠÑĥÐ²Ð¸Ð´ÐµÐ»', 'Ð°Ð²ÑĪÐ¸Ð¹', 'ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸ÑĤÑĮ', 'ĠÑĥÑģÐ¿Ð¾ÐºÐ¾', 'ĠÐ³Ð¾Ð²Ð¾ÑĢÑİ', 'ĠÐ¾ÑĤÐºÑĢÑĭ', 'Ð½ÐµÑģ', 'ĠÑĢÑĥÐºÐ°Ñħ', '0', 'ĠÐ¿Ð¾Ð±', 'ÑĨÐ¸Ð¾Ð½', 'ÑĥÐ¹ÑĤÐµ', 'Ð¸Ñı', 'Ñĩ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", tokenizer.get_vocab_size())\n",
    "print(\"Sample vocab:\", list(tokenizer.get_vocab().keys())[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d220b7",
   "metadata": {},
   "source": [
    "Посмотрим на токенизированный текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f59218a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Тяжкое, тяжкое время, что говорить, - пробормотал он, - но унывать-то\n",
      "['-', 'ĠÐ¢', 'ÑıÐ¶', 'ÐºÐ¾Ðµ', ',', 'ĠÑĤÑıÐ¶', 'ÐºÐ¾Ðµ', 'ĠÐ²ÑĢÐµÐ¼Ñı', ',', 'ĠÑĩÑĤÐ¾', 'ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸ÑĤÑĮ', ',', 'Ġ-', 'ĠÐ¿ÑĢÐ¾Ð±', 'Ð¾ÑĢ', 'Ð¼Ð¾ÑĤÐ°Ð»', 'ĠÐ¾Ð½', ',', 'Ġ-', 'ĠÐ½Ð¾', 'ĠÑĥ', 'Ð½Ñĭ', 'Ð²', 'Ð°ÑĤÑĮ', '-', 'ÑĤÐ¾']\n"
     ]
    }
   ],
   "source": [
    "print (TEST_TEXT)\n",
    "encoding = tokenizer.encode(TEST_TEXT)\n",
    "print (encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593828f1",
   "metadata": {},
   "source": [
    "Посмотрим на 3й токен:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa0e6e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' тяж'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.offsets[5]\n",
    "TEST_TEXT[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b23291",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=SPECIAL_TOKEN,\n",
    "    eos_token=SPECIAL_TOKEN,\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=SPECIAL_TOKEN # Зададим пэддинг-токен как токен конца\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f882f8c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "Токенизируйте данные и подготовьте их к претрейну с длиной контекста 512 токенов в виде экземпляра класса transformers.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7535ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text + SPECIAL_TOKEN for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d8c208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Посвящается Любови Евгеньевне Белозерской<|endoftext|>',\n",
       " 'Пошел мелкий снег и вдруг  повалил  хлопьями.<|endoftext|>',\n",
       " 'Ветер завыл; сделалась метель. В одно  мгновение<|endoftext|>',\n",
       " 'темное  небо  смешалось  с  снежным  морем.  Все<|endoftext|>',\n",
       " 'исчезло.<|endoftext|>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d43893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 52 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if JUST_1ST_DEBUG:\n",
    "    texts = texts[:100]\n",
    "\n",
    "encodings = wrapped_tokenizer(\n",
    "    texts,\n",
    "    add_special_tokens=False,\n",
    "    truncation=False,\n",
    ")\n",
    "all_ids = []\n",
    "for ids in encodings[\"input_ids\"]:\n",
    "    all_ids.extend(ids)\n",
    "\n",
    "chunks = [\n",
    "    all_ids[i:i + CONTEXT_LENGTH]\n",
    "    for i in range(0, len(all_ids), CONTEXT_LENGTH)\n",
    "    if len(all_ids[i:i + CONTEXT_LENGTH]) == CONTEXT_LENGTH\n",
    "]\n",
    "\n",
    "attention_masks = [[1] * CONTEXT_LENGTH for _ in chunks]\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": chunks,\n",
    "    \"attention_mask\": attention_masks,\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7aacea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "Инициализируйте модель ~150M параметров c произвольной decoder-only архитектурой трансформера. Например, можно рассмотреть LlamaConfig с параметрами:\n",
    "- hidden_size=1024, intermediate_size=1536, num_hidden_layers=16, num_attention_heads=16, num_key_value_heads=8.\n",
    "\n",
    "Подготовьте коллбэки для валидации качества на промптах. Реализуйте обучение с помощью Trainer. Обратите внимание на параметры регуляризации weight_decay. Используйте подходящий batch_size — в диапазоне 64—128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9edad79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"Все мысли, которые имеют огромные последствия\",\n",
    "    \"Сила войска зависит от его духа\",\n",
    "    \"Мысль о том, что он принес страдания\",\n",
    "    \"Человек сознает себя свободным\",\n",
    "    \"Что бы ни случилось, я всегда буду\",\n",
    "    \"Любовь мешает смерти\",\n",
    "    \"Нет, жизнь не кончена\",\n",
    "    \"Всякая мысль, даже самая простая\",\n",
    "    \"Война не любезность, а самое гадкое дело\",\n",
    "    \"Чтобы жить честно\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ea7680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель инициализирована: 132.0M параметров\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 102.56 examples/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\Papa\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "100%|██████████| 1/1 [00:10<00:00, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 10.769, 'train_samples_per_second': 0.279, 'train_steps_per_second': 0.093, 'train_loss': 8.186291694641113, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=8.186291694641113, metrics={'train_runtime': 10.769, 'train_samples_per_second': 0.279, 'train_steps_per_second': 0.093, 'train_loss': 8.186291694641113, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    vocab_size=wrapped_tokenizer.vocab_size,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=wrapped_tokenizer.bos_token_id,\n",
    "    eos_token_id=wrapped_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "print(f\"Модель инициализирована: {sum(p.numel() for p in model.parameters())/1e6:.1f}M параметров\")\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=wrapped_tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "class PromptEvalCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompts):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        model.eval()\n",
    "        print(\"\\n=== Валидация на промптах ===\")\n",
    "        for prompt in self.prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Prompt] {prompt}\\n[Output] {text}\")\n",
    "\n",
    "callback = PromptEvalCallback(wrapped_tokenizer, test_prompts)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-small-pretrain\",\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    bf16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=wrapped_tokenizer,\n",
    "    callbacks=[callback],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bb1430",
   "metadata": {},
   "source": [
    "Сгенерируйте ответы на запросы test_prompts. Чтобы ревьюер мог оценить результаты, оставьте генерацию в ноутбуке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f453b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Все мысли, которые имеют огромные последствия\n",
      "Ответ модели: ÐĴ ÑģÐµ ĠÐ¼ÑĭÑģÐ»Ð¸, ĠÐºÐ¾ÑĤÐ¾ÑĢÑĭÐµ ĠÐ¸Ð¼ Ðµ ÑİÑĤ ĠÐ¾Ð³ÑĢÐ¾Ð¼ Ð½ÑĭÐµ ĠÐ¿Ð¾ÑģÐ»ÐµÐ´ ÑģÑĤÐ²Ð¸Ñı ĠÑģÐ²Ð¾ÐµÐ¼ Ð½ÑĥÐ»Ð¸ ÐµÐ¿ÐµÑĢÑĮ ĠÑĢÐ¾ÑĤ Ð¾ÐºÐ¾ ÑģÐ¾Ð² Ġ ĠÐ¾Ð±ÑĢÐ°Ð· ÑĢÑĭ, ĠÑģÐµÐ³Ð¾Ð´Ð½Ñı ÐºÐ°Ñı ĠÑģÐºÐ¾Ð»ÑĮÐºÐ¾ Ð¾Ð»Ðº Ð½ÑĥÐ»Ð¸ Ð°Ñı ÑĢÑĭ Ð°Ñı ĠÐ²Ð·Ð´Ð¾Ñħ Ġ ÑĢÐ¾Ð½ ĠÑģÐ²Ð¾ÐµÐ¼ ĠÐ¿Ð¾Ð´Ð¾Ð± Ð¾ÐºÐ¾ ĠÑģÐ²Ð¾ÐµÐ¼ Ġ ĠÑģÐ²Ð¾ÐµÐ¼ ĠÐ±ÑĭÑģÑĤÑĢÐ¾ ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸Ð»Ð¸ ĠÐ´ÐµÐ»Ð¾ ĠÐ¿Ð¾Ð¶ ÐµÑĢÐ½Ð° ĠÑģÐºÐ¾Ð»ÑĮÐºÐ¾ ÑĮÐ¸, Ġ ĠÐ²Ð·Ð´Ð¾Ñħ ĠÑģÐ¾ ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸Ð»Ð¸ ĠÐ´ÐµÐ»Ð¾ ĠÐ¼ÐµÑģÑı Ð¾Ð´Ð° Ð°ÐºÐ¾Ð¼ ÐµÑĢÐ½Ð° ĠÑģÐ¾ ĠÐ¿Ð¾Ð´Ð¾Ð± ĠÐ¾Ð³Ð»Ñı ÑĪ ÐµÐ´ ĠÑģÐ¾\n",
      "Prompt: Сила войска зависит от его духа\n",
      "Ответ модели: Ð¡ Ð¸Ð»Ð° ĠÐ²Ð¾Ð¹ ÑģÐºÐ° ĠÐ·Ð°Ð² Ð¸Ñģ Ð¸ÑĤ ĠÐ¾ÑĤ ĠÐµÐ³Ð¾ ĠÐ´ÑĥÑħ Ð° Ð»Ð¾Ð² ÐµÑĪÑĮ Ð½Ðµ Ġ ÑĤÑĭ ĠÑģÐ´ ĠÐ¢Ð°Ð¼ Ð¾Ð¹ ĠÑģÐºÐ°Ð¶Ñĥ ÑĢÐ°Ð» ĠÐ± Ð½Ðµ ÑİÑĤ ĠÐ¡ÑĤÐ°ÑĢ ÐµÑĪÑĮ ĠÐ²ÑĭÑħÐ¾Ð´ Ð°Ñĩ ĠÑģÐ»Ð¸ÑĪÐºÐ¾Ð¼ ÐµÑĪÑĮ Ġ ĠÐ·Ð´ÐµÑģÑĮ ĠÑģÐ»Ð¸ÑĪÐºÐ¾Ð¼ ÑĤÑĥ ÐµÐ¿ÐµÑĢÑĮ ÑĪÐµÐµ ĠÐķÑģÐ»Ð¸ ĠÐ²ÑĭÑħÐ¾Ð´ ĠÐ´Ð¾Ð»Ð³Ð¾ ĠÑģÐ´ ÑĢ ĠÐ´Ð¾Ð»Ð³Ð¾ ÑĤÐ°Ð¼ Ġ ĠÐ¡ÑĤÐ°ÑĢ ÑĤÐ°Ð¼ ĠÐºÐ°ÐºÐ¾Ð¹ ĠÑģÐ´ ĠÑģÐ¼Ðµ Ġ ĠÑĨÐ°ÑĢ Ð¼Ð°Ñħ ĠÐ¡ÑĤÐ°ÑĢ ÑĤÑĢ ĠÐ¸Ð´ÑĤÐ¸ ĠÐ¾ÑĤÐº ĠÐ¡ÑĤÐ°ÑĢ ĠÐ´Ð¾Ð»Ð³Ð¾ ĠÐ³ÑĢÐ¾Ð¼ÐºÐ¾ Ð°Ñĩ Ġ\n",
      "Prompt: Мысль о том, что он принес страдания\n",
      "Ответ модели: Ðľ ÑĭÑģÐ» ÑĮ ĠÐ¾ ĠÑĤÐ¾Ð¼, ĠÑĩÑĤÐ¾ ĠÐ¾Ð½ ĠÐ¿ÑĢÐ¸Ð½ÐµÑģ ĠÑģÑĤÑĢÐ°Ð´ Ð°Ð½Ð¸Ñı ĠÑĤÐµÐ± ĠÐ±Ð¾Ð³Ð° ĠÐ»Ð¸ÑĪÑĮ ĠÐºÐ°ÐºÐ¾Ðµ ĠÐ¿Ð¾Ð¹ ĠÐ»Ð¸ÑĪÑĮ Ð°ÑĤÐ° Ð¸Ð½Ð¾Ð¹ ĠÐ¾Ð´Ð½Ð¾Ð³Ð¾ 8 Ð½ÑĭÑħ ĠÑĢÐ°Ð·Ð²Ðµ ĠÐºÐ¾Ð¼Ð½Ð°ÑĤÑĭ ĠÐ±Ð¾Ð»ÐµÐµ Ð·Ð´ Ð°ÑĤÐ° ĥ ÑĢÑĭÐ² ÐĹ ĠÐ¼Ð¾Ð¶ ĠÐ½Ð°Ð· ĠÐ¾Ð±Ð» ĠÐ´ÑĢÑĥÐ³Ð° ĠÐ»ÑİÐ±Ð¾Ð² Ð°ÑĤÐ° ĠÐ²Ð¾Ð·Ð´ÑĥÑħ ĠÑģÑĥÑīÐµ Ð´ÐµÐ¼ ĠÐºÐ¾Ð¼Ð½Ð°ÑĤÑĭ ÑĢÐ°Ð½ÑĨ ĠÐ±Ð¾Ð³Ð° ĠÐ»Ð¸ÑĪÑĮ 8 ĠÑģÑĥÑīÐµ ÑĢÑĭÐ² ĠÐºÐ¾Ð¼Ð½Ð°ÑĤÑĭ ĠÐ²Ð¾Ð·Ð´ÑĥÑħ ÑĢÑĭÐ² ĠÐ¿Ð¾Ð¹ Ð°Ð¼Ð¿ ĠÑģÑĥÑīÐµ ĠÐºÐ¾Ð¼Ð½Ð°ÑĤÑĭ Ð°ÑĤÐ° ĠÐ»Ð¸ÑĪÑĮ Ð¼ÑĥÑī Ð¼ÑĥÑī ĠÐ±Ð¾Ð»ÐµÐµ ĠÑĤÐµÐ± ĠÐ»Ð¸ÑĪÑĮ Ð°Ð·ÑĭÐ²\n",
      "Prompt: Человек сознает себя свободным\n",
      "Ответ модели: Ð§ ÐµÐ»Ð¾Ð² ÐµÐº ĠÑģÐ¾Ð· Ð½Ð° ÐµÑĤ ĠÑģÐµÐ±Ñı ĠÑģÐ²Ð¾Ð±Ð¾Ð´ Ð½ÑĭÐ¼ Ð¾Ð ĠÐ´Ð¾Ð»Ð¶Ð½Ð¾ ĠÐ³ÑĥÑģ ĠÐ¼ÐµÑĢÑĤÐ² ĠÐ¾Ð¿Ð°Ñģ ĠÐ¿ÐµÑĢÐµ ĠÐ¡ÑĤ ĠÐ»Ð¸ÑĪ ĠÐºÐ°ÐºÑĥÑİ ĠÑģÐ¼ÐµÑĪ ĠÐºÐ°ÐºÑĥÑİ ĠÐ¿ÑĢÐ¾ÑģÑĤÐ¾ ĠÐ²ÑĭÐ¿ Ð¸ÑĤÐ° ÑĪÐµ ĠÐ¼Ð¸Ð½ÑĥÑĤ ĠÐ¶ÐµÐ½ÑīÐ¸Ð½Ñĭ ĠÐ´ÑĢÑĥÐ³Ð¾Ð³Ð¾ ĠÐ½Ð°ÑĢÐ¾Ð´ ĠÐ¿Ð»ÐµÑĩ Ð½ÑıÑı ĠÐµÐ´Ð²Ð° Ð°Ð»Ð¾ÑģÑĮ ĠÑĤÑĢÐµÐ± ĠÑĥÐ²Ð¸Ð´ÐµÐ» Ð²Ð¾ ĠÑĪÐµÐ» ÑĢÐ¾Ð´ ĠÐ¾Ð´Ð¸Ð½ ĠÐ²ÑĭÐ¿ ĠÑĪÐµÐ» ÑĢÐ¾Ðº Ð½ÐµÐ¼Ñĥ ĠÐ´ÑĢÑĥÐ³Ð¾Ð³Ð¾ ĠÐĿÑĥ ¦ Ð¸ÑĤÐ° ĠÑĥÐ²Ð¸Ð´ÐµÐ» ĠÐ¼Ð¸Ð½ÑĥÑĤ ÐµÐ¶Ð´ ĠÐ¿ ĠÐ²ÑĭÐ¿ ĠÐ¡ÑĤ ĠÑģÐ¿ÑĢÐ¾Ñģ Ð½ÐµÐ¼Ñĥ ĠÐ²ÑĭÑĪÐµÐ» Ð¸ÑĤÐ° ÐµÐ¶Ð´ ĠÐ¿ÑĢÐ¾Ð² ĠÐĿÑĥ\n",
      "Prompt: Что бы ни случилось, я всегда буду\n",
      "Ответ модели: Ð§ÑĤÐ¾ ĠÐ±Ñĭ ĠÐ½Ð¸ ĠÑģÐ»ÑĥÑĩ Ð¸Ð»Ð¾ÑģÑĮ, ĠÑı ĠÐ²ÑģÐµÐ³Ð´Ð° ĠÐ±ÑĥÐ´Ñĥ ĠÐ²Ð¾ÐºÑĢÑĥÐ³ Ð¾Ð¶Ðµ Ð¸Ð²ÑĪÐ¸ÑģÑĮ ĠÐ±ÐµÑģÐ¿Ð¾ÐºÐ¾Ð¹ ÐµÐ´Ñĥ ĠÐ³Ð¾Ð»Ð¾Ð²Ñĥ ĠÐ²Ð¾ÐºÑĢÑĥÐ³ ĠÐºÐ¾ÑĤÐ¾ÑĢÐ¾Ðµ ÑĢ ĠÐ²ÑģÐµÑħ Ð¸Ð½Ñı Ð¸ÑĤ ÑĢ ÑĢ ĠÐ¿Ð»Ð¾Ñī Ð½ÑĭÐ¹ ÑĥÑģÑĤ ĠÐĲÑĢ Ġ ĠÐ¿Ð¾Ð´Ð½ÑıÐ» Ð ĠÐĿÐ°Ð´Ð¾ ĠÑĥÐ¼ÐµÑĢ Ð³Ð° ĠÐ²Ð¾ÐºÑĢÑĥÐ³ ĠÐĲÑĢ Ð¶Ðµ ÑĮÐµ ÐµÑĤÑĥ Ð Ġ Ð°Ð»Ð¾ÑģÑĮ Ġ ĠÐºÐ°ÐºÐ¾Ð³Ð¾ ÐµÐ´Ñĥ ĠÐ·Ð°Ð² ĠÐ½ Ð»ÐµÐ½Ð° Ġ ĠÑĥÐ¼ÐµÑĢ Ð¸ÑĤ Ð¾Ð»ÑĮÑĤÐµ Ð¾Ð¶Ðµ ÑĮÐµ ÑĮÐµ Ð²Ð¾ÑĢ ĠÐĿÐ°Ð´Ð¾ Ð¼Ð¾ ĠÐºÐ¾ÑĤÐ¾ÑĢÐ¾Ðµ ĠÐ¿Ð¾Ð´Ð½ÑıÐ»\n",
      "Prompt: Любовь мешает смерти\n",
      "Ответ модели: ÐĽ ÑİÐ± Ð¾Ð²ÑĮ ĠÐ¼ÐµÑĪ Ð°ÐµÑĤ ĠÑģÐ¼ÐµÑĢÑĤÐ¸ Ð¾Ð²Ð°ÑĢ ÐºÑĥ ĠÐ¿ÑĢÐ¾Ð´Ð¾Ð»Ð¶Ð°Ð» ĠÐ¿Ð¾Ð» Ð¾Ð»Ñİ Ð¸ÑĨÑĭ ł ĠÐ½ÐµÐ¼ ĠÑĢÐ°Ð´Ð¾ÑģÑĤ ĠÐ¿ÑĢÐ¾Ð´Ð¾Ð»Ð¶Ð°Ð» ĠÐºÐ²Ð°ÑĢÑĤÐ¸ ĠÐ¾Ð±ÑĬ ĭ ĠÐºÐ²Ð°ÑĢÑĤÐ¸ Ð¸ÑĨÑĭ ÑĤÐ¾Ð± ĠÑĩÑĥÐ¶ ĠÐºÑĥÑĢ ÐµÐ½Ð½ÑĭÐ¼ ĠÑĤÐ¾ ÐµÐ½Ð½ÑĭÐ¼ ĠÑĩÑĤÐ¾Ð± ĠÑĩÑĥÐ¶ ĠÐ¿Ð¾Ð´ÑĥÐ¼ ÐµÐºÑĢÐ°Ñģ Ð»Ð¸ÑģÑĮ ĠÐºÐ¾ÑĢ ĠÐ´ÑĢÑĥÐ³Ð° ĠÐ²Ð½Ñĥ ł ĠÐ¿Ð¾Ð» Ð¸ÑĨÑĭ Ð¾Ð»ÑĮ ÑĢÐ°Ð½ ĠÐ¿ÑĢÐ¾ÑģÑĤÐ¾ ĠÐ¿ÑĢÐ¾ÑģÑĤ ĠÐºÐ°ÐºÐ¾Ðµ Ð¸ÑĨÑĭ Ð¸ÑĨÑĭ ĠÑģÐµÐ¹ÑĩÐ°Ñģ ĠÐ¿ÑĢÐ¾ÑģÑĤÐ¾ ĠÐ´ÑĢÑĥÐ³Ð° ĠÐºÐ¾ÑĤÐ¾ÑĢÐ¾Ð¹ ĠÑĩÑĥÐ¶ ĠÐºÐ¾ÑĢ ł ĠÐ¿Ð»Ð°Ðº ÐµÐºÑĢÐ°Ñģ Ð²Ñĭ Ð¸ÑĨÑĭ\n",
      "Prompt: Нет, жизнь не кончена\n",
      "Ответ модели: ÐĿ ÐµÑĤ, ĠÐ¶Ð¸Ð·Ð½ÑĮ ĠÐ½Ðµ ĠÐºÐ¾Ð½Ñĩ ÐµÐ½Ð° Ð°ÑĤÑĮ ĠÐ¼Ð¾ ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸Ð»Ð¸ ĠÐ²Ð¾Ð·Ð¼Ð¾Ð¶ ĠÑįÑĤÐ¾Ð¼Ñĥ ĠÑĪÑĥÑĤ ĠÑģÐ¿Ð¾ÐºÐ¾Ð¹ ĠÑģÐ¾ÑĢ Ð¾Ð½ÑĮ ĠÑįÑĤÐ¾Ð¼Ñĥ ĠÑįÑĤÐ¾Ð¼Ñĥ ÑģÐºÐ¸ ĠÐ¼Ð¾Ð»Ð¾Ð´Ð¾Ð¹ ĠÐ·Ð½Ð°ÑĩÐ¸ÑĤ ĠÑĥÑģÑĤ ĠÐ¼Ð¾ Ġ ĠÐ´Ð¾Ð¶Ð´ ĠÐ¿Ð¾Ð»ÑĥÑĩ ĠÐ¼Ð¾Ð»Ð¾Ð´Ð¾Ð¹ ĠÐ²ÐµÐ» Ð¾Ð½ÑĮ ĠÑģÐ½ ĠÐ½Ð¾Ñĩ ĠÐ½Ð°Ð·ÑĭÐ²\n",
      "Prompt: Всякая мысль, даже самая простая\n",
      "Ответ модели: ÐĴ ÑģÑı ÐºÐ°Ñı ĠÐ¼ÑĭÑģÐ»ÑĮ, ĠÐ´Ð°Ð¶Ðµ ĠÑģÐ°Ð¼ Ð°Ñı ĠÐ¿ÑĢÐ¾ÑģÑĤ Ð°Ñı Ð°Ð½Ð¸ ĠÑģÑĤÐ¾ Ð½ÑıÐ·ÑĮ ĠÐ¿ÐµÑĢÐµÐº ĠÐ³Ð¾ÑģÐ¿Ð¾Ð´Ð° Ð°Ð½Ðµ Ð¾Ð³Ð´Ð° ĠÑĨ ÐµÐ½Ð° ÐµÐ»Ð¾ Ð¾Ð³Ð´Ð° ĠÐ³Ð¾ÑģÐ¿Ð¾Ð´Ð° ÐµÑĤÑĥ ĠÑīÐµÐº ĠÑĨ ĠÐ´Ð° ĠÑĤÐ¾Ð»ÑģÑĤ Ñį ĠÑīÐµÐº Ð¾Ð³Ð´Ð° ĠÐ²ÐµÐ»Ð¸Ðº ĠÐ¸Ð· ĠÐ¸Ð· Ñį Ð¾Ð³Ð´Ð° ĠÐ¼ÑĥÐ· ĠÐ¸Ð· ĠÐ¼ÑĥÐ· ÐºÐ¾Ð² ĠÐ¿Ð¾Ð´Ð¿ ĠÑģÑĤÐ¾ ĠÐ¿Ð¾Ñĩ ĠÐ½ÐµÐ¾Ð±ÑħÐ¾Ð´ ÑĢÐ°Ð½ ÑĢÐ°Ð½ ĠÐ¶Ð¸Ð· Ð±Ð¾ ĠÐ¸Ð· ĠÐµÐ¼Ñĥ ĠÑģÐ¾Ð±Ð¾Ð¹ ĠÐ¾Ð´Ð½Ð¸Ð¼ Ð¾Ð³Ð´Ð° ĠÐłÐ°Ð· ÑģÑĤÐ²Ð¾ ĠÐ¼ÑĥÐ· ÑĪ ĠÑģÐ»ÑĭÑĪÐ°Ð» ĠÐ¾Ð¶ ĠÐ¼ ĠÑģÐ¾Ð±Ð¾Ð¹\n",
      "Prompt: Война не любезность, а самое гадкое дело\n",
      "Ответ модели: ÐĴ Ð¾Ð¹ Ð½Ð° ĠÐ½Ðµ ĠÐ»ÑİÐ± ÐµÐ· Ð½Ð¾ÑģÑĤÑĮ, ĠÐ° ĠÑģÐ°Ð¼Ð¾Ðµ ĠÐ³ Ð°Ð´ ÐºÐ¾Ðµ ĠÐ´ÐµÐ»Ð¾ ÐºÐ° Ð¾Ð·Ñı ĠÐŃÑĤÐ¾ ĠÐľÐ°ÑĢ Ð°Ð¹ÑĤÐµ ÐºÐ° ĠÑģÐ¾ ĠÐ¼Ð³Ð½Ð¾Ð² Ð¾Ð·Ñı ĠÑģÐ¾ ĠÑģÐ¾Ñģ ĠÐłÐ°Ð· ĠÑģÐ¾ ĠÑħÐ¾Ñĩ ÑĢÐ°Ð·Ñĥ ĠÑħÐ¾Ñĩ Ð¸Ð»Ð¾Ð² ÑĥÐ½Ð´ ĠÑģÑĤÐ¾ ĠÐºÐ°Ð¶ ĠÑıÑĢ ĠÑģÐ¾ ĠÐ»ÑĥÑĩ ĠÐ½ÐµÐ± Ð¾Ð·Ñı ĠÐ»ÑĥÑĩ Ð¾Ð·Ñı ÐºÐ° ÐµÐ»ÑģÑı ĠÐ²Ð·Ð³Ð»ÑıÐ½ÑĥÐ» ĠÑģÐ¾ÑĤ ÑĢÐ¾ÑģÐ¸Ð» Ð¾ÑģÑĤ ĠÑģÐ¾ ÑĢÐ°Ð·Ñĥ ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸ÑĤ ĠÑģÐ¾Ñģ ĠÑģÐ¾ ĠÑģÐ¿ ĠÐ»ÑĥÑĩ Ð°Ð¹ ĠÐ´Ð¾Ð³ ĠÐ³Ð¾Ð²Ð¾ÑĢÐ¸ÑĤ ÑĢÐ°Ð½ Ð¸Ð»Ð¾Ð² ĠÐ¿ÑĮÑı ÑĢÐ¾ÑģÐ¸Ð» Ð°Ñı ĠÐ·Ð°Ð¶ ÑĤÐµÑĢ\n",
      "Prompt: Чтобы жить честно\n",
      "Ответ модели: Ð§ ÑĤÐ¾Ð± Ñĭ ĠÐ¶Ð¸ÑĤÑĮ ĠÑĩ ÐµÑģÑĤÐ½Ð¾ Ð½Ð¾Ðµ ĠÐ²Ð´ ĠÐ¿Ð°Ð´ ĠÐ¶Ð¸Ð» ĠÑĤÐµÑħ Ð¾Ð¼Ð¸Ð½ Ð°Ñī ÑĤÐµÑĢÐµÑģ ĠÐ¼Ð°Ð»ÑĮ ĠÐĲÑħ ĠÐ°Ð½ Ð¾Ð³Ñĥ ĠÑģÐµÐ» Ð½Ð¾Ðµ Ð°Ñī ĠÑįÑĤÐ¾ÑĤ ĠÐ·Ð°Ð¿Ð°Ñħ ĠÑģÐ»Ð°Ð´ ĠÐ±Ð¾Ð³Ð° ĠÐºÐ°ÐºÐ¾Ð³Ð¾ Ð¾Ð³Ñĥ ĠÐ³Ð»Ð°Ð· ĠÐ¿Ð¾ÑĩÐµÐ¼Ñĥ ĠÐ¤ Ð¾Ð³Ñĥ ĠÐ¼Ð°Ð»ÑĮ ĠÐ²Ð´ÑĢÑĥÐ³ Ð°Ñī ĠÐ²Ð´ÑĢÑĥÐ³ Ð¾Ð³Ñĥ ĠÐ²Ð´ÑĢÑĥÐ³ ĠÑģÐµÐ» ÑĥÐ± ĠÐ³Ð»Ð°Ð· ĠÐ¼Ð°Ñħ ĠÐ¿ÑĢÐ¸Ñĩ ĠÐ¼Ð°Ñħ ĠÐ²ÐµÑī ĠÐ»Ð¾ÑĪÐ°Ð´ÐµÐ¹ ĠÐ³Ð»Ð°Ð· ĠÐ½Ð°ÐºÐ»Ð¾Ð½ ÑĤ ĠÐ½Ð°ÐºÐ»Ð¾Ð½ ĠÐ¿ÑĢÐ¸Ñĩ ĠÐ²Ð´ÑĢÑĥÐ³ ĠÐ±Ð» Ð½ÐµÐ· ĠÑĩ ĠÐ¼Ð°Ñħ Ð»Ð¾Ð²\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    inputs = wrapped_tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Удаляем token_type_ids если они есть\n",
    "    if 'token_type_ids' in inputs:\n",
    "        del inputs['token_type_ids']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.8,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            pad_token_id=wrapped_tokenizer.pad_token_id,\n",
    "            eos_token_id=wrapped_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    text = wrapped_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\\nОтвет модели: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1e4a5c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Post-train SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca13cfa7",
   "metadata": {},
   "source": [
    "Для SFT-этапа можно использовать значительно меньше данных, поэтому возьмём модель крупнее. Рассмотрим базовую модель Qwen2.5-0.5B, с которой вы встречались в уроках. Обучите её генерировать ответы на инструктивные русскоязычные вопросы.\n",
    "\n",
    "Для оценки качества используйте такой набор вопросов:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f56437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_rus = [\n",
    "    \"сколько планет в нашей солнечной системе?\",\n",
    "    \"расскажи стих\",\n",
    "    \"когда собирать крыжовник?\",\n",
    "    \"Как быстро выучить новый язык?\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68263aa9",
   "metadata": {},
   "source": [
    "Подготовьте инструктивный датасет [d0rj/alpaca-cleaned-ru](https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru) для обучения базовой модели. Представьте его в виде transformers.Dataset для диалоговых данных input=system, instruction=user, output=assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000652d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae61ac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'instruction', 'output'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(SFT_DATASET, split=\"train\")\n",
    "if JUST_1ST_DEBUG:\n",
    "    ds = ds.select(range(100))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5962f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'instruction': 'Дайте три совета, как оставаться здоровым.',\n",
       " 'output': '1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\\n\\n2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее значение для поддержания крепких костей, мышц и здоровья сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут умеренным аэробным упражнениям или 75 минут интенсивным упражнениям каждую неделю.\\n\\n3. Высыпайтесь. Достаточное количество качественного сна имеет решающее значение для физического и психического благополучия. Он помогает регулировать настроение, улучшать когнитивные функции и поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 часов каждую ночь.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "167bca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 3031.73 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'Вы — доброжелательный помощник.', 'role': 'system'}, {'content': 'Дайте три совета, как оставаться здоровым.', 'role': 'user'}, {'content': '1. Соблюдайте сбалансированную и питательную диету. Убедитесь, что в ваш рацион входят разнообразные фрукты и овощи, нежирный белок, цельнозерновые продукты и полезные жиры. Это помогает обеспечить ваш организм необходимыми питательными веществами для оптимального функционирования и может помочь предотвратить хронические заболевания.\\n\\n2. Занимайтесь регулярной физической активностью. Упражнения имеют решающее значение для поддержания крепких костей, мышц и здоровья сердечно-сосудистой системы. Старайтесь уделять не менее 150 минут умеренным аэробным упражнениям или 75 минут интенсивным упражнениям каждую неделю.\\n\\n3. Высыпайтесь. Достаточное количество качественного сна имеет решающее значение для физического и психического благополучия. Он помогает регулировать настроение, улучшать когнитивные функции и поддерживает здоровый рост и иммунную функцию. Старайтесь спать 7-9 часов каждую ночь.', 'role': 'assistant'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": example.get(\"input\", \"\") or \"Вы — доброжелательный помощник.\"},\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "ds = ds.map(to_messages)\n",
    "\n",
    "print(ds[0][\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788903d3",
   "metadata": {},
   "source": [
    "С помощью trl SFTTrainer дообучите модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81dcf7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classic_sft(ds):\n",
    "    model_id = SFT_MODEL\n",
    "    tok = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\")\n",
    "    cfg = SFTConfig(\n",
    "        output_dir=\"full_sft\",\n",
    "        per_device_train_batch_size=1,  \n",
    "        logging_steps=1,\n",
    "        max_length=1024,\n",
    "        num_train_epochs=1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        learning_rate=5e-5,\n",
    "        run_name='SFT'\n",
    "    )\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=cfg,\n",
    "        train_dataset=ds,\n",
    "        processing_class=tok,\n",
    "    )\n",
    "    trainer.train()\n",
    "    trainer.save_model(FINETUNED_MODEL_PATH)\n",
    "    tok.save_pretrained(FINETUNED_MODEL_PATH)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0148461",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = run_classic_sft(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e5d418",
   "metadata": {},
   "source": [
    "Посмотрите на адекватность генерации на финальных данных. \n",
    "Факты могут быть ошибочными, но язык ответа должен быть русским и должна сохраняться структура ответа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tok, question, max_new_tokens=200):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Вы — доброжелательный помощник.\"},\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "\n",
    "    text = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tok(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tok.pad_token_id,\n",
    "            eos_token_id=tok.eos_token_id\n",
    "        )\n",
    "\n",
    "    answer = tok.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#    if question in answer:\n",
    "#        answer = answer.split(question, 1)[-1].strip()\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c65406",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in questions_rus:\n",
    "    print(\"------------------------------\")\n",
    "    print(f\"Вопрос: {q}\\n\")\n",
    "    print(\"Ответ: {generate_answer(model, tok, q)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicumNLP-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
