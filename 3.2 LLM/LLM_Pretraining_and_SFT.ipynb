{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ce0391",
   "metadata": {},
   "source": [
    "# Проект: pretraining LLM и posttraining LLM\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "- Pretraining\n",
    "    - Претрейн — самый ресурсоёмкий этап обучения LLM. Чтобы полноценно обучить даже небольшую модель (менее 1B), понадобится более 10к GPU-часов на A100. Чтобы не тратить недели на обучение, но отработать ключевые приёмы, в проекте вы выполните упрощённую задачу. \n",
    "    - При полноценном претрейне модель учится обобщать знания из данных, на которых происходило обучение, чтобы потом извлекать эти знания по текстовым запросам уже после обучения. Упростим задачу — научим модель только структуре языка. \n",
    "    - Сосредоточимся на одном узком домене — **текстах произведений русской литературы** — и **обучим модель продолжать фразы из этого домена разумным текстом**. \n",
    "- Posttraining\n",
    "    - Для SFT-этапа можно использовать значительно меньше данных, поэтому возьмём модель крупнее. Рассмотрим базовую модель **Qwen2.5-0.5B**, с которой вы встречались в уроках. **Обучите её генерировать ответы на инструктивные русскоязычные вопросы**.\n",
    "\n",
    "## Критерии успеха\n",
    "- Pretraining - чтобы оценить качество, используйте промпты:\n",
    "    - \"Все мысли, которые имеют огромные последствия\",\n",
    "    - \"Сила войска зависит от его духа\",\n",
    "    - \"Мысль о том, что он принес страдания\",\n",
    "    - \"Человек сознает себя свободным\",\n",
    "    - \"Что бы ни случилось, я всегда буду\",\n",
    "    - \"Любовь мешает смерти\",\n",
    "    - \"Нет, жизнь не кончена\",\n",
    "    - \"Всякая мысль, даже самая простая\",\n",
    "    - \"Война не любезность, а самое гадкое дело\",\n",
    "    - \"Чтобы жить честно\"\n",
    "- Posttraining - качество данных должно быть сопоставимо с таким вариантом::\n",
    "    - Model Input 1:\n",
    "    сколько планет в нашей солнечной системе?\n",
    "        - Model Output 1:\n",
    "    Согласно последним исследованиям, Солнце имеет 8 планеты. Это дает планетам, которые соответствуют разным критериям, таким как диета, местоположение, плотность, климат и другие.\n",
    "    assistant\n",
    "    Общее размер нашей Солнечной системы составляет примерно 9,9 миллиарда километров, а в результате на каждом из них есть планета,\n",
    "    - Model Input 2:\n",
    "    расскажи стих\n",
    "        - Model Output 2:\n",
    "    Вот сладкий сладкий вкус, который ты получаешь на себе\n",
    "    Когда я говорю, что я люблю тебя\n",
    "    Мое сердце, у меня есть все, что я хочу, чтобы сделать\n",
    "    У меня есть все, что мне нужно, чтобы быть хорошим\n",
    "    Так давай пойдем в ресторан\n",
    "    assistant\n",
    "    Пока мы уседим, что мы можем\n",
    "    И в этот момент мы возвращаемся к при\n",
    "    - Model Input 3:\n",
    "    когда собирать крыжовник?\n",
    "        - Model Output 3:\n",
    "    Когда собираешь крыжовник, важно не беспокоиться о том, что другие могут его хвастаться. Вместо этого проверяйте свои способности и готовность. Вы можете изучить свой талант и стремления, прежде чем принимать решение, и признавать, что у вас есть свои сильные стороны, а также слабые места. Не снимайте крыжовник в обществе, и обязательно помните о\n",
    "    - Model Input 4:\n",
    "    Как быстро выучить новый язык?\n",
    "        - Model Output 4:\n",
    "    Скорость усвоения нового языка может сильно различаться в зависимости от нескольких факторов, таких как скорость, с которой вы пакетируете информацию через ее и ее способность учиться в различных условиях. Тем не менее, вот некоторые общие рекомендации: 1. Делайте сценарии. Обычно это может занять от 2 до 4 лет, чтобы понять основные концепции. Вы можете начать с"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343bf23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\" style=\"background-color:#D9EEE1;color:black;\">\n",
    "\n",
    "## Описание данных\n",
    "\n",
    "- Pretrain\n",
    "    - https://github.com/JoannaBy/RussianNovels/tree/master/corpus \n",
    "- Posttrain\n",
    "    - русскоязычный инструктивный датасет [d0rj/alpaca-cleaned-ru](https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru) в диалоговом формате"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905d42a",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fd01d",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc2623a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: GitPython in c:\\users\\papa\\appdata\\roaming\\python\\python311\\site-packages (3.1.45)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\papa\\appdata\\roaming\\python\\python311\\site-packages (from GitPython) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\papa\\appdata\\roaming\\python\\python311\\site-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install GitPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from git import Repo\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "from tokenizers.models import BPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b78884",
   "metadata": {},
   "source": [
    "### Установка главных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e48b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_DATASET =  \"pretrain-dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f98b8",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f277c58",
   "metadata": {},
   "source": [
    "Скачайте данные из репозитория и упакуйте их в один датасет. Вам понадобятся все произведения из репозитория.\n",
    "\n",
    "Проведите препроцессинг данных:\n",
    "- Очистите их от дубликатов.\n",
    "- Очистите от предложений с буквами не из кириллицы.\n",
    "- Обработайте повторяющуюся пунктуацию и т. д.\n",
    "- Разбейте на чанки поменьше, чтобы можно было добавить <bos> и <eos> токены в соответствии с обучаемой длиной контекста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2aff8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(PRETRAIN_DATASET):\n",
    "    Repo.clone_from(\"https://github.com/JoannaBy/RussianNovels.git\", PRETRAIN_DATASET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedDataset(Dataset):\n",
    "    def __init__(self, text_files_dir):\n",
    "        self.texts = []\n",
    "        for file_path in Path(text_files_dir).glob(\"*.txt\"):\n",
    "            with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    self.texts.append(line.strip())\n",
    "        self.clean_data()\n",
    "        \n",
    "    def clean_data(self):\n",
    "        # удалим строки с недопустимыми символами\n",
    "        allowed_pattern = re.compile(r\"^[А-Яа-яЁё0-9.,!?;:\\-()\\\"'\\s]+$\")\n",
    "        self.texts = [t for t in self.texts if allowed_pattern.fullmatch(t)]\n",
    "\n",
    "        # Заменяем повторяющиеся знаки пунктуации на один\n",
    "        self.texts = [re.sub(r'([.,!?;:\\-()])\\1+', r'\\1', t) for t in self.texts]\n",
    "\n",
    "        #удалим дубликаты\n",
    "        self.texts = pd.DataFrame({'t': self.texts}).drop_duplicates()['t'].tolist()\n",
    "\n",
    "        #перенесём строки, если они длиннее 300 слов\n",
    "        max_words = 300 # из рассчёта, что ~1.5 токена на слово, а токенов у нас в контексте 512\n",
    "        result = []\n",
    "\n",
    "        for text in self.texts:\n",
    "            words = text.split()\n",
    "            if len(words) > max_words:\n",
    "                parts = [' '.join(words[i:i + max_words]) \\\n",
    "                            for i in range(0, len(words), max_words)]\n",
    "                result.extend(parts)\n",
    "            else:\n",
    "                result.append(text)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"text\": self.texts[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eff1b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dataset = PretrainedDataset(Path(PRETRAIN_DATASET, \"corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4454f1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270498"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21f3d605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '- Тяжкое, тяжкое время, что говорить, - пробормотал он, - но унывать-то'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dataset.__getitem__(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77515878",
   "metadata": {},
   "source": [
    "Создайте и обучите собственный токенизатор на полученных данных. Размер словаря выберите небольшим: при обучении только на рассмотренных текстах — около 3к токенов. В рассматриваемых данных язык намного менее разнообразен, чем в совокупных данных, поэтому крупные токенизаторы от реальных LLM могут не подойти. \n",
    "При создании токенизатора можете ориентироваться на [материал huggingface.co](https://huggingface.co/learn/llm-course/ru/chapter6/8). Рекомендуем использовать BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e324527",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicumNLP-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
