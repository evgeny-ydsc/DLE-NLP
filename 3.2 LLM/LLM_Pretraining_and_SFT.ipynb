{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ce0391",
   "metadata": {},
   "source": [
    "# –ü—Ä–æ–µ–∫—Ç: pretraining LLM –∏ posttraining LLM\n",
    "\n",
    "## –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏\n",
    "\n",
    "- Pretraining\n",
    "    - –ü—Ä–µ—Ç—Ä–µ–π–Ω ‚Äî —Å–∞–º—ã–π —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–∏–π —ç—Ç–∞–ø –æ–±—É—á–µ–Ω–∏—è LLM. –ß—Ç–æ–±—ã –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ –æ–±—É—á–∏—Ç—å –¥–∞–∂–µ –Ω–µ–±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å (–º–µ–Ω–µ–µ 1B), –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –±–æ–ª–µ–µ 10–∫ GPU-—á–∞—Å–æ–≤ –Ω–∞ A100. –ß—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –Ω–µ–¥–µ–ª–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–µ, –Ω–æ –æ—Ç—Ä–∞–±–æ—Ç–∞—Ç—å –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–∏—ë–º—ã, –≤ –ø—Ä–æ–µ–∫—Ç–µ –≤—ã –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –∑–∞–¥–∞—á—É. \n",
    "    - –ü—Ä–∏ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–º –ø—Ä–µ—Ç—Ä–µ–π–Ω–µ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –æ–±–æ–±—â–∞—Ç—å –∑–Ω–∞–Ω–∏—è –∏–∑ –¥–∞–Ω–Ω—ã—Ö, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–æ –æ–±—É—á–µ–Ω–∏–µ, —á—Ç–æ–±—ã –ø–æ—Ç–æ–º –∏–∑–≤–ª–µ–∫–∞—Ç—å —ç—Ç–∏ –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–∞–º —É–∂–µ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è. –£–ø—Ä–æ—Å—Ç–∏–º –∑–∞–¥–∞—á—É ‚Äî –Ω–∞—É—á–∏–º –º–æ–¥–µ–ª—å —Ç–æ–ª—å–∫–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —è–∑—ã–∫–∞. \n",
    "    - –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á–∏–º—Å—è –Ω–∞ –æ–¥–Ω–æ–º —É–∑–∫–æ–º –¥–æ–º–µ–Ω–µ ‚Äî **—Ç–µ–∫—Å—Ç–∞—Ö –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–π —Ä—É—Å—Å–∫–æ–π –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã** ‚Äî –∏ **–æ–±—É—á–∏–º –º–æ–¥–µ–ª—å –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Ñ—Ä–∞–∑—ã –∏–∑ —ç—Ç–æ–≥–æ –¥–æ–º–µ–Ω–∞ —Ä–∞–∑—É–º–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º**. \n",
    "- Posttraining\n",
    "    - –î–ª—è SFT-—ç—Ç–∞–ø–∞ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–Ω—å—à–µ –¥–∞–Ω–Ω—ã—Ö, –ø–æ—ç—Ç–æ–º—É –≤–æ–∑—å–º—ë–º –º–æ–¥–µ–ª—å –∫—Ä—É–ø–Ω–µ–µ. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å **Qwen2.5-0.5B**, —Å –∫–æ—Ç–æ—Ä–æ–π –≤—ã –≤—Å—Ç—Ä–µ—á–∞–ª–∏—Å—å –≤ —É—Ä–æ–∫–∞—Ö. **–û–±—É—á–∏—Ç–µ –µ—ë –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–µ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã**.\n",
    "\n",
    "## –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞\n",
    "- Pretraining - —á—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ–º–ø—Ç—ã:\n",
    "    - \"–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\",\n",
    "    - \"–°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\",\n",
    "    - \"–ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\",\n",
    "    - \"–ß–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\",\n",
    "    - \"–ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\",\n",
    "    - \"–õ—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\",\n",
    "    - \"–ù–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\",\n",
    "    - \"–í—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\",\n",
    "    - \"–í–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\",\n",
    "    - \"–ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\"\n",
    "- Posttraining - –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏–º–æ —Å —Ç–∞–∫–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º::\n",
    "    - Model Input 1:\n",
    "    —Å–∫–æ–ª—å–∫–æ –ø–ª–∞–Ω–µ—Ç –≤ –Ω–∞—à–µ–π —Å–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ?\n",
    "        - Model Output 1:\n",
    "    –°–æ–≥–ª–∞—Å–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è–º, –°–æ–ª–Ω—Ü–µ –∏–º–µ–µ—Ç 8 –ø–ª–∞–Ω–µ—Ç—ã. –≠—Ç–æ –¥–∞–µ—Ç –ø–ª–∞–Ω–µ—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ä–∞–∑–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º, —Ç–∞–∫–∏–º –∫–∞–∫ –¥–∏–µ—Ç–∞, –º–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ, –ø–ª–æ—Ç–Ω–æ—Å—Ç—å, –∫–ª–∏–º–∞—Ç –∏ –¥—Ä—É–≥–∏–µ.\n",
    "    assistant\n",
    "    –û–±—â–µ–µ —Ä–∞–∑–º–µ—Ä –Ω–∞—à–µ–π –°–æ–ª–Ω–µ—á–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ 9,9 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –∫–∏–ª–æ–º–µ—Ç—Ä–æ–≤, –∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –Ω–∞ –∫–∞–∂–¥–æ–º –∏–∑ –Ω–∏—Ö –µ—Å—Ç—å –ø–ª–∞–Ω–µ—Ç–∞,\n",
    "    - Model Input 2:\n",
    "    —Ä–∞—Å—Å–∫–∞–∂–∏ —Å—Ç–∏—Ö\n",
    "        - Model Output 2:\n",
    "    –í–æ—Ç —Å–ª–∞–¥–∫–∏–π —Å–ª–∞–¥–∫–∏–π –≤–∫—É—Å, –∫–æ—Ç–æ—Ä—ã–π —Ç—ã –ø–æ–ª—É—á–∞–µ—à—å –Ω–∞ —Å–µ–±–µ\n",
    "    –ö–æ–≥–¥–∞ —è –≥–æ–≤–æ—Ä—é, —á—Ç–æ —è –ª—é–±–ª—é —Ç–µ–±—è\n",
    "    –ú–æ–µ —Å–µ—Ä–¥—Ü–µ, —É –º–µ–Ω—è –µ—Å—Ç—å –≤—Å–µ, —á—Ç–æ —è —Ö–æ—á—É, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å\n",
    "    –£ –º–µ–Ω—è –µ—Å—Ç—å –≤—Å–µ, —á—Ç–æ –º–Ω–µ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –±—ã—Ç—å —Ö–æ—Ä–æ—à–∏–º\n",
    "    –¢–∞–∫ –¥–∞–≤–∞–π –ø–æ–π–¥–µ–º –≤ —Ä–µ—Å—Ç–æ—Ä–∞–Ω\n",
    "    assistant\n",
    "    –ü–æ–∫–∞ –º—ã —É—Å–µ–¥–∏–º, —á—Ç–æ –º—ã –º–æ–∂–µ–º\n",
    "    –ò –≤ —ç—Ç–æ—Ç –º–æ–º–µ–Ω—Ç –º—ã –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–∏\n",
    "    - Model Input 3:\n",
    "    –∫–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞—Ç—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫?\n",
    "        - Model Output 3:\n",
    "    –ö–æ–≥–¥–∞ —Å–æ–±–∏—Ä–∞–µ—à—å –∫—Ä—ã–∂–æ–≤–Ω–∏–∫, –≤–∞–∂–Ω–æ –Ω–µ –±–µ—Å–ø–æ–∫–æ–∏—Ç—å—Å—è –æ —Ç–æ–º, —á—Ç–æ –¥—Ä—É–≥–∏–µ –º–æ–≥—É—Ç –µ–≥–æ —Ö–≤–∞—Å—Ç–∞—Ç—å—Å—è. –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –ø—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å–≤–æ–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å. –í—ã –º–æ–∂–µ—Ç–µ –∏–∑—É—á–∏—Ç—å —Å–≤–æ–π —Ç–∞–ª–∞–Ω—Ç –∏ —Å—Ç—Ä–µ–º–ª–µ–Ω–∏—è, –ø—Ä–µ–∂–¥–µ —á–µ–º –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ, –∏ –ø—Ä–∏–∑–Ω–∞–≤–∞—Ç—å, —á—Ç–æ —É –≤–∞—Å –µ—Å—Ç—å —Å–≤–æ–∏ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã, –∞ —Ç–∞–∫–∂–µ —Å–ª–∞–±—ã–µ –º–µ—Å—Ç–∞. –ù–µ —Å–Ω–∏–º–∞–π—Ç–µ –∫—Ä—ã–∂–æ–≤–Ω–∏–∫ –≤ –æ–±—â–µ—Å—Ç–≤–µ, –∏ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –ø–æ–º–Ω–∏—Ç–µ –æ\n",
    "    - Model Input 4:\n",
    "    –ö–∞–∫ –±—ã—Å—Ç—Ä–æ –≤—ã—É—á–∏—Ç—å –Ω–æ–≤—ã–π —è–∑—ã–∫?\n",
    "        - Model Output 4:\n",
    "    –°–∫–æ—Ä–æ—Å—Ç—å —É—Å–≤–æ–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ —è–∑—ã–∫–∞ –º–æ–∂–µ—Ç —Å–∏–ª—å–Ω–æ —Ä–∞–∑–ª–∏—á–∞—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤, —Ç–∞–∫–∏—Ö –∫–∞–∫ —Å–∫–æ—Ä–æ—Å—Ç—å, —Å –∫–æ—Ç–æ—Ä–æ–π –≤—ã –ø–∞–∫–µ—Ç–∏—Ä—É–µ—Ç–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é —á–µ—Ä–µ–∑ –µ–µ –∏ –µ–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —É—á–∏—Ç—å—Å—è –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—Å–ª–æ–≤–∏—è—Ö. –¢–µ–º –Ω–µ –º–µ–Ω–µ–µ, –≤–æ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: 1. –î–µ–ª–∞–π—Ç–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏. –û–±—ã—á–Ω–æ —ç—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –æ—Ç 2 –¥–æ 4 –ª–µ—Ç, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏. –í—ã –º–æ–∂–µ—Ç–µ –Ω–∞—á–∞—Ç—å —Å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343bf23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\" style=\"background-color:#D9EEE1;color:black;\">\n",
    "\n",
    "## –û–ø–∏—Å–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "- Pretrain\n",
    "    - https://github.com/JoannaBy/RussianNovels/tree/master/corpus \n",
    "- Posttrain\n",
    "    - —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç [d0rj/alpaca-cleaned-ru](https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru) –≤ –¥–∏–∞–ª–æ–≥–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905d42a",
   "metadata": {},
   "source": [
    "## –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fd01d",
   "metadata": {},
   "source": [
    "### –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2623a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install GitPython -q\n",
    "!pip install accelerate==0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb23077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from git import Repo\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer, pre_tokenizers, trainers\n",
    "from transformers import (\n",
    "    LlamaConfig,\n",
    "    LlamaForCausalLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from tokenizers.models import BPE\n",
    "from transformers import LlamaConfig, LlamaForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b78884",
   "metadata": {},
   "source": [
    "### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≥–ª–∞–≤–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0e48b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_DATASET =  \"pretrain-dataset\"\n",
    "DICT_SIZE = 3000\n",
    "SPECIAL_TOKEN = \"<|endoftext|>\"\n",
    "CONTEXT_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f98b8",
   "metadata": {},
   "source": [
    "# Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f277c58",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "\n",
    "–°–∫–∞—á–∞–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏ —É–ø–∞–∫—É–π—Ç–µ –∏—Ö –≤ –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç. –í–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è –≤—Å–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∏–∑ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è.\n",
    "\n",
    "–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö:\n",
    "- –û—á–∏—Å—Ç–∏—Ç–µ –∏—Ö –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.\n",
    "- –û—á–∏—Å—Ç–∏—Ç–µ –æ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –±—É–∫–≤–∞–º–∏ –Ω–µ –∏–∑ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã.\n",
    "- –û–±—Ä–∞–±–æ—Ç–∞–π—Ç–µ –ø–æ–≤—Ç–æ—Ä—è—é—â—É—é—Å—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ç. –¥.\n",
    "- –†–∞–∑–±–µ–π—Ç–µ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ–º–µ–Ω—å—à–µ, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –¥–æ–±–∞–≤–∏—Ç—å <bos> –∏ <eos> —Ç–æ–∫–µ–Ω—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –æ–±—É—á–∞–µ–º–æ–π –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2aff8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(PRETRAIN_DATASET):\n",
    "    Repo.clone_from(\"https://github.com/JoannaBy/RussianNovels.git\", PRETRAIN_DATASET) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "929d9c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for file_path in Path(Path(PRETRAIN_DATASET, \"corpus\")).glob(\"*.txt\"):\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            texts.append(line.strip())\n",
    "\n",
    "# —É–¥–∞–ª–∏–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏\n",
    "allowed_pattern = re.compile(r\"^[–ê-–Ø–∞-—è–Å—ë0-9.,!?;:\\-()\\\"'\\s]+$\")\n",
    "texts = [t for t in texts if allowed_pattern.fullmatch(t)]\n",
    "\n",
    "# –ó–∞–º–µ–Ω—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –Ω–∞ –æ–¥–∏–Ω\n",
    "texts = [re.sub(r'([.,!?;:\\-()])\\1+', r'\\1', t) for t in texts]\n",
    "\n",
    "#—É–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "texts = pd.DataFrame({'t': texts}).drop_duplicates()['t'].tolist()\n",
    "\n",
    "#–ø–µ—Ä–µ–Ω–µ—Å—ë–º —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –¥–ª–∏–Ω–Ω–µ–µ 300 —Å–ª–æ–≤\n",
    "max_words = 300 # –∏–∑ —Ä–∞—Å—Å—á—ë—Ç–∞, —á—Ç–æ ~1.5 —Ç–æ–∫–µ–Ω–∞ –Ω–∞ —Å–ª–æ–≤–æ, –∞ —Ç–æ–∫–µ–Ω–æ–≤ —É –Ω–∞—Å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ 512\n",
    "result = []\n",
    "for text in texts:\n",
    "    words = text.split()\n",
    "    if len(words) > max_words:\n",
    "        parts = [' '.join(words[i:i + max_words]) \\\n",
    "                    for i in range(0, len(words), max_words)]\n",
    "        result.extend(parts)\n",
    "    else:\n",
    "        result.append(text)\n",
    "texts = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21f3d605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- –¢—è–∂–∫–æ–µ, —Ç—è–∂–∫–æ–µ –≤—Ä–µ–º—è, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—å, - –ø—Ä–æ–±–æ—Ä–º–æ—Ç–∞–ª –æ–Ω, - –Ω–æ —É–Ω—ã–≤–∞—Ç—å-—Ç–æ'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEST_TEXT = texts[100]\n",
    "TEST_TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77515878",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "–°–æ–∑–¥–∞–π—Ç–µ –∏ –æ–±—É—á–∏—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–∞ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –≤—ã–±–µ—Ä–∏—Ç–µ –Ω–µ–±–æ–ª—å—à–∏–º: –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö ‚Äî –æ–∫–æ–ª–æ 3–∫ —Ç–æ–∫–µ–Ω–æ–≤. –í —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö —è–∑—ã–∫ –Ω–∞–º–Ω–æ–≥–æ –º–µ–Ω–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–µ–Ω, —á–µ–º –≤ —Å–æ–≤–æ–∫—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –ø–æ—ç—Ç–æ–º—É –∫—Ä—É–ø–Ω—ã–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä—ã –æ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö LLM –º–æ–≥—É—Ç –Ω–µ –ø–æ–¥–æ–π—Ç–∏. \n",
    "–ü—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –º–æ–∂–µ—Ç–µ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ [–º–∞—Ç–µ—Ä–∏–∞–ª huggingface.co](https://huggingface.co/learn/llm-course/ru/chapter6/8). –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea73b56e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', (0, 1)),\n",
       " ('ƒ†√ê¬¢√ëƒ±√ê¬∂√ê¬∫√ê¬æ√ê¬µ', (1, 8)),\n",
       " (',', (8, 9)),\n",
       " ('ƒ†√ëƒ§√ëƒ±√ê¬∂√ê¬∫√ê¬æ√ê¬µ', (9, 16)),\n",
       " ('ƒ†√ê¬≤√ëƒ¢√ê¬µ√ê¬º√ëƒ±', (16, 22)),\n",
       " (',', (22, 23)),\n",
       " ('ƒ†√ëƒ©√ëƒ§√ê¬æ', (23, 27)),\n",
       " ('ƒ†√ê¬≥√ê¬æ√ê¬≤√ê¬æ√ëƒ¢√ê¬∏√ëƒ§√ëƒÆ', (27, 36)),\n",
       " (',', (36, 37)),\n",
       " ('ƒ†-', (37, 39)),\n",
       " ('ƒ†√ê¬ø√ëƒ¢√ê¬æ√ê¬±√ê¬æ√ëƒ¢√ê¬º√ê¬æ√ëƒ§√ê¬∞√ê¬ª', (39, 51)),\n",
       " ('ƒ†√ê¬æ√ê¬Ω', (51, 54)),\n",
       " (',', (54, 55)),\n",
       " ('ƒ†-', (55, 57)),\n",
       " ('ƒ†√ê¬Ω√ê¬æ', (57, 60)),\n",
       " ('ƒ†√ëƒ•√ê¬Ω√ëƒ≠√ê¬≤√ê¬∞√ëƒ§√ëƒÆ', (60, 68)),\n",
       " ('-', (68, 69)),\n",
       " ('√ëƒ§√ê¬æ', (69, 71))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer.pre_tokenize_str(TEST_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2ed49b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(texts):\n",
    "    for i in range(0, len(texts), 1000):\n",
    "        batch = [texts[j] for j in range(i, min(i + 1000, len(texts)))]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e324527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 59s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = trainers.BpeTrainer(vocab_size=DICT_SIZE, special_tokens=[SPECIAL_TOKEN])\n",
    "tokenizer.train_from_iterator(get_training_corpus(texts), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c681be0",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "253297be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 3000\n",
      "Sample vocab: ['ƒ†√ê¬¥√ê¬æ', 'ƒ†√ê¬≤√ê¬µ√ëƒ¢√ê¬Ω√ê¬æ', 'ƒ†√ê¬ª√ëƒ∞√ê¬±√ê¬æ√ê¬≤', '√ê¬æ√ê¬∂', 'ƒ†√ê¬Ω√ê¬µ√ëƒ£√ëƒ©√ê¬∞√ëƒ£√ëƒ§', 'ƒ†√ëƒ¢√ëƒ•√ê¬±', '√ê¬∏√ê¬≤√ê¬∞√ê¬µ√ëƒ§', 'ƒ†√ê¬≥√ëƒ•√ê¬±√ëƒ≠', 'ƒ†√ê¬≥√ê¬æ√ê¬≤√ê¬æ√ëƒ¢√ê¬∏√ëƒ§√ëƒÆ', '√ê¬Ω√ëƒ±√ê¬∑√ëƒÆ', 'ƒ†√ê¬º√ê¬∏√ê¬Ω√ëƒ•√ëƒ§', '√ëƒØ√ëƒ§', '√ê¬æ√ê¬∂√ëƒ•', 'ƒ†√ê¬∏√ëƒß', '√ëƒ¢√ê¬∏√ê¬π', '?\"', '√ê¬æ√ê¬ª√ê¬æ', '√ê¬æ√ëƒ£√ê¬∫√ê¬≤', 'ƒ†√ëƒ£√ê¬ª√ê¬æ√ê¬≤√ê¬∞', 'ƒ†√ê¬ø√ëƒ¢√ê¬∏√ê¬Ω', '¬∏', 'ƒ†√ê¬°√ëƒ§√ê¬µ√ê¬ø', 'ƒ†√ê¬ø√ê¬æ√ê¬∫√ê¬∞√ê¬∑√ê¬∞√ê¬ª√ê¬æ√ëƒ£√ëƒÆ', 'ƒ†√ëƒ¢√ê¬µ√ê¬∑', '√ê¬æ√ê¬ª√ëƒÆ√ê¬Ω√ê¬æ', 'ƒ†√ëƒ©√ê¬∞√ëƒ£√ëƒ§√ê¬æ', 'ƒ†√ê¬∑√ê¬≤', '√ê¬§', '√ê¬∫√ê¬Ω√ëƒ•√ê¬ª√ê¬∞', 'ƒ†√ê≈É√ëƒ§√ê¬æ']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab size:\", tokenizer.get_vocab_size())\n",
    "print(\"Sample vocab:\", list(tokenizer.get_vocab().keys())[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d220b7",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f59218a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- –¢—è–∂–∫–æ–µ, —Ç—è–∂–∫–æ–µ –≤—Ä–µ–º—è, —á—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç—å, - –ø—Ä–æ–±–æ—Ä–º–æ—Ç–∞–ª –æ–Ω, - –Ω–æ —É–Ω—ã–≤–∞—Ç—å-—Ç–æ\n",
      "['-', 'ƒ†√ê¬¢', '√ëƒ±√ê¬∂', '√ê¬∫√ê¬æ√ê¬µ', ',', 'ƒ†√ëƒ§√ëƒ±√ê¬∂', '√ê¬∫√ê¬æ√ê¬µ', 'ƒ†√ê¬≤√ëƒ¢√ê¬µ√ê¬º√ëƒ±', ',', 'ƒ†√ëƒ©√ëƒ§√ê¬æ', 'ƒ†√ê¬≥√ê¬æ√ê¬≤√ê¬æ√ëƒ¢√ê¬∏√ëƒ§√ëƒÆ', ',', 'ƒ†-', 'ƒ†√ê¬ø√ëƒ¢√ê¬æ√ê¬±', '√ê¬æ√ëƒ¢', '√ê¬º√ê¬æ√ëƒ§√ê¬∞√ê¬ª', 'ƒ†√ê¬æ√ê¬Ω', ',', 'ƒ†-', 'ƒ†√ê¬Ω√ê¬æ', 'ƒ†√ëƒ•', '√ê¬Ω√ëƒ≠', '√ê¬≤', '√ê¬∞√ëƒ§√ëƒÆ', '-', '√ëƒ§√ê¬æ']\n"
     ]
    }
   ],
   "source": [
    "print (TEST_TEXT)\n",
    "encoding = tokenizer.encode(TEST_TEXT)\n",
    "print (encoding.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593828f1",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ 3–π —Ç–æ–∫–µ–Ω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa0e6e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' —Ç—è–∂'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start, end = encoding.offsets[5]\n",
    "TEST_TEXT[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91b23291",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=SPECIAL_TOKEN,\n",
    "    eos_token=SPECIAL_TOKEN,\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=SPECIAL_TOKEN # –ó–∞–¥–∞–¥–∏–º –ø—ç–¥–¥–∏–Ω–≥-—Ç–æ–∫–µ–Ω –∫–∞–∫ —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f882f8c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∏—Ö –∫ –ø—Ä–µ—Ç—Ä–µ–π–Ω—É —Å –¥–ª–∏–Ω–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ 512 —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–∏–¥–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∫–ª–∞—Å—Å–∞ transformers.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7535ed24",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [text + SPECIAL_TOKEN for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31d8c208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['–ü–æ—Å–≤—è—â–∞–µ—Ç—Å—è –õ—é–±–æ–≤–∏ –ï–≤–≥–µ–Ω—å–µ–≤–Ω–µ –ë–µ–ª–æ–∑–µ—Ä—Å–∫–æ–π<|endoftext|>',\n",
       " '–ü–æ—à–µ–ª –º–µ–ª–∫–∏–π —Å–Ω–µ–≥ –∏ –≤–¥—Ä—É–≥  –ø–æ–≤–∞–ª–∏–ª  —Ö–ª–æ–ø—å—è–º–∏.<|endoftext|>',\n",
       " '–í–µ—Ç–µ—Ä –∑–∞–≤—ã–ª; —Å–¥–µ–ª–∞–ª–∞—Å—å –º–µ—Ç–µ–ª—å. –í –æ–¥–Ω–æ  –º–≥–Ω–æ–≤–µ–Ω–∏–µ<|endoftext|>',\n",
       " '—Ç–µ–º–Ω–æ–µ  –Ω–µ–±–æ  —Å–º–µ—à–∞–ª–æ—Å—å  —Å  —Å–Ω–µ–∂–Ω—ã–º  –º–æ—Ä–µ–º.  –í—Å–µ<|endoftext|>',\n",
       " '–∏—Å—á–µ–∑–ª–æ.<|endoftext|>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122345f0",
   "metadata": {},
   "source": [
    "üë∑üößüößüößüößüöß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53d43893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 250 ms\n",
      "Wall time: 84 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 49\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "encodings = wrapped_tokenizer(\n",
    "    texts[:1000], # –í–†–ï–ú–ï–ù–û!!!!!!!!!!!!!!!!!!!!!!!! üë∑üößüößüößüößüöß\n",
    "    add_special_tokens=False,\n",
    "    truncation=False,\n",
    ")\n",
    "all_ids = []\n",
    "for ids in encodings[\"input_ids\"]:\n",
    "    all_ids.extend(ids)\n",
    "\n",
    "chunks = [\n",
    "    all_ids[i:i + CONTEXT_LENGTH]\n",
    "    for i in range(0, len(all_ids), CONTEXT_LENGTH)\n",
    "    if len(all_ids[i:i + CONTEXT_LENGTH]) == CONTEXT_LENGTH\n",
    "]\n",
    "\n",
    "attention_masks = [[1] * CONTEXT_LENGTH for _ in chunks]\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": chunks,\n",
    "    \"attention_mask\": attention_masks,\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7aacea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\">\n",
    "\n",
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å ~150M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ c –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–π decoder-only –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, –º–æ–∂–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å LlamaConfig —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:\n",
    "- hidden_size=1024, intermediate_size=1536, num_hidden_layers=16, num_attention_heads=16, num_key_value_heads=8.\n",
    "\n",
    "–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∫–æ–ª–ª–±—ç–∫–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–∞ –ø—Ä–æ–º–ø—Ç–∞—Ö. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Trainer. –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ weight_decay. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–π batch_size ‚Äî –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 64‚Äî128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ea7680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: 132.0M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:00<00:00, 1385.26 examples/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 58\u001b[0m\n\u001b[0;32m     43\u001b[0m test_prompts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m–ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m ] \n\u001b[0;32m     56\u001b[0m callback \u001b[38;5;241m=\u001b[39m PromptEvalCallback(wrapped_tokenizer, test_prompts)\n\u001b[1;32m---> 58\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./llama-small-pretrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#evaluation_strategy=\"steps\",\u001b[39;49;00m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbf16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     76\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     77\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[callback],\n\u001b[0;32m     83\u001b[0m )\n\u001b[0;32m     85\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m<string>:131\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[0m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\practicumNLP-py311\\Lib\\site-packages\\transformers\\training_args.py:1738\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[1;32m-> 1738\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[0;32m   1741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maverage_tokens_across_devices:\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\practicumNLP-py311\\Lib\\site-packages\\transformers\\training_args.py:2268\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2264\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;124;03mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   2266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m-> 2268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\practicumNLP-py311\\Lib\\site-packages\\transformers\\utils\\generic.py:67\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     65\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\practicumNLP-py311\\Lib\\site-packages\\transformers\\training_args.py:2138\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   2137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[1;32m-> 2138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m   2139\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2141\u001b[0m         )\n\u001b[0;32m   2142\u001b[0m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[0;32m   2143\u001b[0m accelerator_state_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menabled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_configured_state\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
     ]
    }
   ],
   "source": [
    "config = LlamaConfig(\n",
    "    vocab_size=wrapped_tokenizer.vocab_size,\n",
    "    hidden_size=1024,\n",
    "    intermediate_size=1536,\n",
    "    num_hidden_layers=16,\n",
    "    num_attention_heads=16,\n",
    "    num_key_value_heads=8,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=wrapped_tokenizer.bos_token_id,\n",
    "    eos_token_id=wrapped_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM(config)\n",
    "print(f\"–ú–æ–¥–µ–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞: {sum(p.numel() for p in model.parameters())/1e6:.1f}M –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "\n",
    "dataset = dataset.map(lambda x: {\"labels\": x[\"input_ids\"]})\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.05, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=wrapped_tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "class PromptEvalCallback(TrainerCallback):\n",
    "    def __init__(self, tokenizer, prompts):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompts = prompts\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        model.eval()\n",
    "        print(\"\\n=== –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –ø—Ä–æ–º–ø—Ç–∞—Ö ===\")\n",
    "        for prompt in self.prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**inputs, max_new_tokens=30)\n",
    "            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(f\"\\n[Prompt] {prompt}\\n[Output] {text}\")\n",
    "\n",
    "test_prompts = [\n",
    "    \"–í—Å–µ –º—ã—Å–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ–≥—Ä–æ–º–Ω—ã–µ –ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏—è\",\n",
    "    \"–°–∏–ª–∞ –≤–æ–π—Å–∫–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –µ–≥–æ –¥—É—Ö–∞\",\n",
    "    \"–ú—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –æ–Ω –ø—Ä–∏–Ω–µ—Å —Å—Ç—Ä–∞–¥–∞–Ω–∏—è\",\n",
    "    \"–ß–µ–ª–æ–≤–µ–∫ —Å–æ–∑–Ω–∞–µ—Ç —Å–µ–±—è —Å–≤–æ–±–æ–¥–Ω—ã–º\",\n",
    "    \"–ß—Ç–æ –±—ã –Ω–∏ —Å–ª—É—á–∏–ª–æ—Å—å, —è –≤—Å–µ–≥–¥–∞ –±—É–¥—É\",\n",
    "    \"–õ—é–±–æ–≤—å –º–µ—à–∞–µ—Ç —Å–º–µ—Ä—Ç–∏\",\n",
    "    \"–ù–µ—Ç, –∂–∏–∑–Ω—å –Ω–µ –∫–æ–Ω—á–µ–Ω–∞\",\n",
    "    \"–í—Å—è–∫–∞—è –º—ã—Å–ª—å, –¥–∞–∂–µ —Å–∞–º–∞—è –ø—Ä–æ—Å—Ç–∞—è\",\n",
    "    \"–í–æ–π–Ω–∞ –Ω–µ –ª—é–±–µ–∑–Ω–æ—Å—Ç—å, –∞ —Å–∞–º–æ–µ –≥–∞–¥–∫–æ–µ –¥–µ–ª–æ\",\n",
    "    \"–ß—Ç–æ–±—ã –∂–∏—Ç—å —á–µ—Å—Ç–Ω–æ\"\n",
    "] \n",
    "\n",
    "callback = PromptEvalCallback(wrapped_tokenizer, test_prompts)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama-small-pretrain\",\n",
    "    per_device_train_batch_size=64,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",\n",
    "    bf16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=wrapped_tokenizer,\n",
    "    callbacks=[callback],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a3d040",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicumNLP-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
