{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ce0391",
   "metadata": {},
   "source": [
    "# Проект: pretrain и posttrain LLM\n",
    "\n",
    "## Постановка задачи\n",
    "\n",
    "- Pretrain\n",
    "    - Претрейн — самый ресурсоёмкий этап обучения LLM. Чтобы полноценно обучить даже небольшую модель (менее 1B), понадобится более 10к GPU-часов на A100. Чтобы не тратить недели на обучение, но отработать ключевые приёмы, в проекте вы выполните упрощённую задачу. \n",
    "    - При полноценном претрейне модель учится обобщать знания из данных, на которых происходило обучение, чтобы потом извлекать эти знания по текстовым запросам уже после обучения. Упростим задачу — научим модель только структуре языка. \n",
    "    - Сосредоточимся на одном узком домене — **текстах произведений русской литературы** — и **обучим модель продолжать фразы из этого домена разумным текстом**. \n",
    "- Posttrain\n",
    "    - Для SFT-этапа можно использовать значительно меньше данных, поэтому возьмём модель крупнее. Рассмотрим базовую модель **Qwen2.5-0.5B**, с которой вы встречались в уроках. **Обучите её генерировать ответы на инструктивные русскоязычные вопросы**.\n",
    "\n",
    "## Критерии успеха\n",
    "- Pretrain - чтобы оценить качество, используйте промпты:\n",
    "    - \"Все мысли, которые имеют огромные последствия\",\n",
    "    - \"Сила войска зависит от его духа\",\n",
    "    - \"Мысль о том, что он принес страдания\",\n",
    "    - \"Человек сознает себя свободным\",\n",
    "    - \"Что бы ни случилось, я всегда буду\",\n",
    "    - \"Любовь мешает смерти\",\n",
    "    - \"Нет, жизнь не кончена\",\n",
    "    - \"Всякая мысль, даже самая простая\",\n",
    "    - \"Война не любезность, а самое гадкое дело\",\n",
    "    - \"Чтобы жить честно\"\n",
    "- Posttrain - качество данных должно быть сопоставимо с таким вариантом::\n",
    "    - Model Input 1:\n",
    "    сколько планет в нашей солнечной системе?\n",
    "        - Model Output 1:\n",
    "    Согласно последним исследованиям, Солнце имеет 8 планеты. Это дает планетам, которые соответствуют разным критериям, таким как диета, местоположение, плотность, климат и другие.\n",
    "    assistant\n",
    "    Общее размер нашей Солнечной системы составляет примерно 9,9 миллиарда километров, а в результате на каждом из них есть планета,\n",
    "    - Model Input 2:\n",
    "    расскажи стих\n",
    "        - Model Output 2:\n",
    "    Вот сладкий сладкий вкус, который ты получаешь на себе\n",
    "    Когда я говорю, что я люблю тебя\n",
    "    Мое сердце, у меня есть все, что я хочу, чтобы сделать\n",
    "    У меня есть все, что мне нужно, чтобы быть хорошим\n",
    "    Так давай пойдем в ресторан\n",
    "    assistant\n",
    "    Пока мы уседим, что мы можем\n",
    "    И в этот момент мы возвращаемся к при\n",
    "    - Model Input 3:\n",
    "    когда собирать крыжовник?\n",
    "        - Model Output 3:\n",
    "    Когда собираешь крыжовник, важно не беспокоиться о том, что другие могут его хвастаться. Вместо этого проверяйте свои способности и готовность. Вы можете изучить свой талант и стремления, прежде чем принимать решение, и признавать, что у вас есть свои сильные стороны, а также слабые места. Не снимайте крыжовник в обществе, и обязательно помните о\n",
    "    - Model Input 4:\n",
    "    Как быстро выучить новый язык?\n",
    "        - Model Output 4:\n",
    "    Скорость усвоения нового языка может сильно различаться в зависимости от нескольких факторов, таких как скорость, с которой вы пакетируете информацию через ее и ее способность учиться в различных условиях. Тем не менее, вот некоторые общие рекомендации: 1. Делайте сценарии. Обычно это может занять от 2 до 4 лет, чтобы понять основные концепции. Вы можете начать с"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4343bf23",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-secondary\" style=\"background-color:#D9EEE1;color:black;\">\n",
    "\n",
    "## Описание данных\n",
    "\n",
    "- Pretrain\n",
    "    - https://github.com/JoannaBy/RussianNovels/tree/master/corpus \n",
    "- Posttrain\n",
    "    - русскоязычный инструктивный датасет [d0rj/alpaca-cleaned-ru](https://huggingface.co/datasets/d0rj/alpaca-cleaned-ru) в диалоговом формате"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e905d42a",
   "metadata": {},
   "source": [
    "## Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fd01d",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23077c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07b78884",
   "metadata": {},
   "source": [
    "### Установка главных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0e48b31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nENCODER_NAME = \"cointegrated/rubert-tiny2\"\\n#ENCODER_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\\nTOKENIZER_MAX_LEN = 512\\nRELATIONS_MAX_LEN = 30 # TOP отношений\\nEPOCHS_MAX = 3000\\nLEARNING_RATE = 5e-5\\n##v2 LEARNING_RATE = 5e-4 - укрупнение шага не помогло\\nTEXT_LR = 1e-5\\n##v2 TEXT_LR = 1e-4 - укрупнение шага не помогло\\nDROPOUT = 0.1\\nWEIGHT_DECAY = 1e-4\\nSCHEDULER_T_MAX = 10\\nSCHEDULER_ETA_MIN = 1e-6\\nBATCH_SIZE = 32\\nTARGET_TOKEN_MACRO_F1 = 0.5\\nTARGET_CLS_MICRO_F1 = 0.8\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "ENCODER_NAME = \"cointegrated/rubert-tiny2\"\n",
    "#ENCODER_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "TOKENIZER_MAX_LEN = 512\n",
    "RELATIONS_MAX_LEN = 30 # TOP отношений\n",
    "EPOCHS_MAX = 3000\n",
    "LEARNING_RATE = 5e-5\n",
    "##v2 LEARNING_RATE = 5e-4 - укрупнение шага не помогло\n",
    "TEXT_LR = 1e-5\n",
    "##v2 TEXT_LR = 1e-4 - укрупнение шага не помогло\n",
    "DROPOUT = 0.1\n",
    "WEIGHT_DECAY = 1e-4\n",
    "SCHEDULER_T_MAX = 10\n",
    "SCHEDULER_ETA_MIN = 1e-6\n",
    "BATCH_SIZE = 32\n",
    "TARGET_TOKEN_MACRO_F1 = 0.5\n",
    "TARGET_CLS_MICRO_F1 = 0.8\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff8ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practicumNLP-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
