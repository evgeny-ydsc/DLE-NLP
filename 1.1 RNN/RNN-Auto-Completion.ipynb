{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdznZNyZ36t5"
      },
      "source": [
        "# Проект Автодополнение текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMYC9T6i36t_"
      },
      "source": [
        "Вы работаете в соцсетевом приложении, где пользователи постят короткие тексты. В продукте стоит задача — добавить возможность автодополнения текстов. Разработчики просят вас создать модель, которую можно запускать на мобильных устройствах. Для смартфонов есть значительные требования по оперативной памяти и скорости работы, так что важна легковесность модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onS-SYto36uA"
      },
      "source": [
        "## Постановка задачи\n",
        "**Создать нейросеть, которая на основе начала фразы предсказывает её продолжение**.\n",
        "\n",
        "Поэтапное описание задачи:\n",
        "1. Взять датасет от разработчиков, очистить его, подготовить для обучения модели.\n",
        "1. Реализовать и обучить модель на основе рекуррентных нейронных сетей.\n",
        "1. Замерить качество разработанной и обученной модели.\n",
        "1. Взять более «тяжёлую» предобученную модель из Transformers и замерить её качество.\n",
        "1. Проанализировать результаты и **дать рекомендации** разработчикам: стоит ли **использовать лёгкую модель или** лучше постараться поработать с ограничениями по памяти и **использовать большую предобученную**.\n",
        "\n",
        "## Критерии успеха\n",
        "- Используемые метрики качества: ROUGE-1 и ROUGE-2\n",
        "- Минимальных порогов качества модели в задаче не задано\n",
        "\n",
        "\n",
        "## Описание данных\n",
        "\n",
        "датасет с короткими постами sentiment140\n",
        "\n",
        "https://code.s3.yandex.net/deep-learning/tweets.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_xJyxMe36uC"
      },
      "source": [
        "# Этап 0. Подготовка окружения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58t6n_5C36uC"
      },
      "source": [
        "Актуальный код функций доступен тут:\n",
        "\n",
        "https://github.com/evgeny-ydsc/DLE-NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HSgu8AM958A8"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate -q\n",
        "!pip install rouge_score -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZOCK9GdBvrIg"
      },
      "outputs": [],
      "source": [
        "IS_GOOGLE_COLAB=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9mYYBcv36uF",
        "outputId": "43fa7779-1da0-4270-900c-500428c03d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import evaluate\n",
        "\n",
        "if IS_GOOGLE_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    sys.path.append('/content/drive/MyDrive/project1')\n",
        "else:\n",
        "    sys.path.append('./')\n",
        "\n",
        "from src.next_token_dataset import prepare_loaders\n",
        "from src.lstm_model import LSTMGenerator\n",
        "from src.lstm_train_eval import ModelTrainer\n",
        "from src.data_utils import download_dataset\n",
        "from src.eval_transformer_pipeline import Distilgpt2Generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6oGdnoF36uF"
      },
      "source": [
        "# Этап 1. Сбор и подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kVdZRlh1vrIh"
      },
      "outputs": [],
      "source": [
        "if IS_GOOGLE_COLAB:\n",
        "    MAX_TEXTS_COUNT = 100_000\n",
        "else: # просто для черновой отладки локально на CPU\n",
        "    MAX_TEXTS_COUNT = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLAhrBpA36uG",
        "outputId": "f75a2828-7b69-4e66-b944-4954ac7ba185"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "if IS_GOOGLE_COLAB:\n",
        "    raw_data_path = \"drive/MyDrive/project1/data/tweets.txt\"\n",
        "else: # просто для черновой отладки локально на CPU\n",
        "    raw_data_path = \"data/tweets.txt\"\n",
        "\n",
        "download_dataset(\"https://code.s3.yandex.net/deep-learning/tweets.txt\",\n",
        "                                 raw_data_path)\n",
        "train_loader, val_loader, test_loader, tokenizer, val_texts, test_texts = \\\n",
        "    prepare_loaders (raw_data_path, batch_size=256, max_texts_count=MAX_TEXTS_COUNT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZsE690936uH"
      },
      "source": [
        "# Этап 2. Реализация рекуррентной сети\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X6tt04Vq36uH"
      },
      "outputs": [],
      "source": [
        "model = LSTMGenerator(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36Rr_B8I36uH"
      },
      "source": [
        "# Этап 3. Тренировка модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWXb38sr36uH",
        "outputId": "dc95dcc7-9eff-4106-af35-ce8f5d217502"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3737/3737 [00:38<00:00, 97.99it/s]\n",
            "Evaluating ROUGE: 100%|██████████| 10000/10000 [01:41<00:00, 98.95it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Train Loss: 6.028 | Val Loss: 5.644 | Val Accuracy: 15.77%| rouge1: 0.039 | rouge2: 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3737/3737 [00:39<00:00, 94.87it/s]\n",
            "Evaluating ROUGE: 100%|██████████| 10000/10000 [01:40<00:00, 99.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Train Loss: 5.344 | Val Loss: 5.531 | Val Accuracy: 16.95%| rouge1: 0.035 | rouge2: 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3737/3737 [00:38<00:00, 97.81it/s] \n",
            "Evaluating ROUGE: 100%|██████████| 10000/10000 [01:41<00:00, 98.61it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Train Loss: 5.066 | Val Loss: 5.549 | Val Accuracy: 17.44%| rouge1: 0.044 | rouge2: 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3737/3737 [00:39<00:00, 95.22it/s] \n",
            "Evaluating ROUGE: 100%|██████████| 10000/10000 [01:40<00:00, 99.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Train Loss: 4.868 | Val Loss: 5.592 | Val Accuracy: 17.73%| rouge1: 0.038 | rouge2: 0.004\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3737/3737 [00:38<00:00, 97.99it/s]\n",
            "Evaluating ROUGE: 100%|██████████| 10000/10000 [01:41<00:00, 98.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Train Loss: 4.711 | Val Loss: 5.650 | Val Accuracy: 17.83%| rouge1: 0.033 | rouge2: 0.004\n"
          ]
        }
      ],
      "source": [
        "trainer = ModelTrainer (model, tokenizer)\n",
        "trainer.train(train_loader, val_loader, val_texts, n_epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7aKCFZZ44u"
      },
      "source": [
        "~~Видно, что начиная с 5й эпохи модель начала переобучатсья. Но GPU Практикума недоступна, а ресурс colab тоже уже закончился и не гарантирован (уже обрывался по полпути ранее). Поэтому не рискую его перезапускать ещё раз, оставлю модель как есть в этот раз.~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54_yjEAH36uJ",
        "outputId": "a22cf7ae-b728-4061-f1c4-0096fd416b60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------\n",
            "оригинальная строка: addicted to dramasi have to stop watching them continuallymake self tired and not doing things have to do\n",
            "оригинальное начало: addicted to dramasi have to stop watching them continuallymake self tired and \n",
            "оригинальный конец : not doing things have to do\n",
            "предсказанный конец: i have to go to work tomorrow morning and i have to go to work tomorrow morning and i have to go to work tomorrow morning and\n",
            "rouge1=0.121, rouge2=0.065\n",
            "\n",
            "----------------------------------------------\n",
            "оригинальная строка: mesirii no adhoc meeting spontaneously turned into code camp\n",
            "оригинальное начало: mesirii no adhoc meeting spontaneously turned\n",
            "оригинальный конец :  into code camp\n",
            "предсказанный конец: out to the beach and i have to go to work tomorrow morning and i\n",
            "rouge1=0.000, rouge2=0.000\n",
            "\n",
            "----------------------------------------------\n",
            "оригинальная строка: i wish my computer wasnt broken so i could get a effin pic upwhy cant i do it on my blackberrry\n",
            "оригинальное начало: i wish my computer wasnt broken so i could get a effin pic upwhy cant i\n",
            "оригинальный конец :  do it on my blackberrry\n",
            "предсказанный конец: have to go to work tomorrow morning and i have to go to work tomorrow morning and i have to go to work tomorrow\n",
            "rouge1=0.000, rouge2=0.000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "for line in test_texts[:3]:\n",
        "    l = len(line)\n",
        "    i = int(l*0.75)\n",
        "    start = line [:i]\n",
        "    finish = line [i:]\n",
        "    predicted_finish = model.generate_output_text(tokenizer, start, l-i)\n",
        "    results = rouge.compute(predictions=[predicted_finish], references=[finish])\n",
        "    print (f\"----------------------------------------------\")\n",
        "    print (f\"оригинальная строка: {line}\")\n",
        "    print (f\"оригинальное начало: {start}\")\n",
        "    print (f\"оригинальный конец : {finish}\")\n",
        "    print (f\"предсказанный конец: {predicted_finish}\")\n",
        "    print (f\"rouge1={results['rouge1']:.3f}, rouge2={results['rouge2']:.3f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rn47i2BG36uK"
      },
      "source": [
        "# Этап 4. Использование предобученного трансформера"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjDWuQ_W36uK",
        "outputId": "aac12608-e25f-496f-accc-1a9672dead1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "bert_model = Distilgpt2Generator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_4SsWQdvrIk"
      },
      "source": [
        "div class=\"alert alert-info\" style=\"background-color:#FFF8DC;color:black;\">\n",
        "<b>🎓 Комментарии студента v1:</b>\n",
        "\n",
        "Нашёл ошибку в коде с трансформером - строка обрезалась дважды - и в функции generate_output_text, и в сниппете ниже [l:] - поправил.\n",
        "\n",
        "Поэтому и выдача трансформера была такая странная."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLLhtrSx36uK",
        "outputId": "fe0e2ff7-145b-42d0-8f06-cb70cc248fa8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=27) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=15) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------\n",
            "оригинальная строка: addicted to dramasi have to stop watching them continuallymake self tired and not doing things have to do\n",
            "оригинальное начало: addicted to dramasi have to stop watching them continuallymake self tired and \n",
            "оригинальный конец : not doing things have to do\n",
            "предсказанный конец: icky.\n",
            "rouge1=0.000, rouge2=0.000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=24) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------\n",
            "оригинальная строка: mesirii no adhoc meeting spontaneously turned into code camp\n",
            "оригинальное начало: mesirii no adhoc meeting spontaneously turned\n",
            "оригинальный конец :  into code camp\n",
            "предсказанный конец:  into a violent clash.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rouge1=0.286, rouge2=0.000\n",
            "----------------------------------------------\n",
            "оригинальная строка: i wish my computer wasnt broken so i could get a effin pic upwhy cant i do it on my blackberrry\n",
            "оригинальное начало: i wish my computer wasnt broken so i could get a effin pic upwhy cant i\n",
            "оригинальный конец :  do it on my blackberrry\n",
            "предсказанный конец:  make my computer. I have been able to do it for over a year now and dont have any problems with it. I think i will always be able to do it.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "rouge1=0.162, rouge2=0.057\n",
            "rouge1=0.149, rouge2=0.019\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predictions=[]\n",
        "references=[]\n",
        "\n",
        "for line in test_texts[:3]:\n",
        "    l = len(line)\n",
        "    i = int(l*0.75)\n",
        "    start = line [:i]\n",
        "    finish = line [i:]\n",
        "    predicted_finish = bert_model.generate_output_text(start, l-i)#v1[l:]\n",
        "    results = rouge.compute(predictions=[predicted_finish], references=[finish])\n",
        "    print (f\"----------------------------------------------\")\n",
        "    print (f\"оригинальная строка: {line}\")\n",
        "    print (f\"оригинальное начало: {start}\")\n",
        "    print (f\"оригинальный конец : {finish}\")\n",
        "    print (f\"предсказанный конец: {predicted_finish}\")\n",
        "    print (f\"rouge1={results['rouge1']:.3f}, rouge2={results['rouge2']:.3f}\")\n",
        "\n",
        "    predictions.append(predicted_finish)\n",
        "    references.append(finish)\n",
        "\n",
        "results = rouge.compute(predictions=predictions, references=references)\n",
        "print (f\"rouge1={results['rouge1']:.3f}, rouge2={results['rouge2']:.3f}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XVXq5g336uL"
      },
      "source": [
        "# Этап 5. Формулирование выводов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1-1nptU36uL"
      },
      "source": [
        "Была обучена модель на основе LSTM. Показатели на лучшем тексте из трёх: rouge1=0.121, rouge2=0.065\n",
        "\n",
        "Также был использован предобученный трансформер, усреднённые показатели: rouge1=0.149, rouge2=0.019\n",
        "\n",
        "Из этого следует, что использование предобученного трансформера предпочтительнее полученной LSTM-модели."
      ]
    }
  ],
  "metadata": {
    "ExecuteTimeLog": [
      {
        "duration": 45,
        "start_time": "2025-04-13T00:31:37.601Z"
      },
      {
        "duration": 6,
        "start_time": "2025-04-13T00:35:12.169Z"
      },
      {
        "duration": 3185,
        "start_time": "2025-04-16T20:05:37.378Z"
      },
      {
        "duration": 7396,
        "start_time": "2025-04-16T20:06:11.392Z"
      },
      {
        "duration": 1458,
        "start_time": "2025-04-16T20:07:06.981Z"
      },
      {
        "duration": 3384,
        "start_time": "2025-04-16T20:07:10.928Z"
      },
      {
        "duration": 2634,
        "start_time": "2025-04-16T20:07:14.314Z"
      },
      {
        "duration": 3233,
        "start_time": "2025-04-16T20:07:16.950Z"
      },
      {
        "duration": 4,
        "start_time": "2025-04-16T20:07:21.523Z"
      },
      {
        "duration": 15,
        "start_time": "2025-04-16T20:07:22.279Z"
      },
      {
        "duration": 1126,
        "start_time": "2025-04-16T20:07:24.193Z"
      },
      {
        "duration": 170,
        "start_time": "2025-04-16T20:07:27.323Z"
      },
      {
        "duration": 343,
        "start_time": "2025-04-16T20:07:28.598Z"
      },
      {
        "duration": 2,
        "start_time": "2025-04-16T20:07:28.943Z"
      },
      {
        "duration": 30,
        "start_time": "2025-04-16T20:07:28.982Z"
      },
      {
        "duration": 135,
        "start_time": "2025-04-16T20:07:29.974Z"
      },
      {
        "duration": 15,
        "start_time": "2025-04-16T20:07:30.273Z"
      },
      {
        "duration": 15,
        "start_time": "2025-04-16T20:07:30.425Z"
      },
      {
        "duration": 172,
        "start_time": "2025-04-16T20:07:30.924Z"
      },
      {
        "duration": 119,
        "start_time": "2025-04-16T20:07:31.757Z"
      },
      {
        "duration": 1209,
        "start_time": "2025-04-16T20:07:32.102Z"
      },
      {
        "duration": 493,
        "start_time": "2025-04-16T20:07:33.313Z"
      },
      {
        "duration": 173,
        "start_time": "2025-04-16T20:07:34.241Z"
      },
      {
        "duration": 30,
        "start_time": "2025-04-16T20:07:34.685Z"
      },
      {
        "duration": 138,
        "start_time": "2025-04-16T20:07:35.869Z"
      },
      {
        "duration": 10897,
        "start_time": "2025-04-16T20:07:36.199Z"
      },
      {
        "duration": 1619,
        "start_time": "2025-04-16T20:07:47.105Z"
      },
      {
        "duration": 362,
        "start_time": "2025-04-16T20:07:48.726Z"
      },
      {
        "duration": 42,
        "start_time": "2025-04-16T20:07:49.090Z"
      },
      {
        "duration": 32,
        "start_time": "2025-04-16T20:07:49.134Z"
      },
      {
        "duration": 29,
        "start_time": "2025-04-16T20:07:53.399Z"
      },
      {
        "duration": 27,
        "start_time": "2025-04-16T20:07:55.160Z"
      },
      {
        "duration": 6,
        "start_time": "2025-04-16T20:07:57.128Z"
      },
      {
        "duration": 165,
        "start_time": "2025-04-16T20:07:59.059Z"
      },
      {
        "duration": 28,
        "start_time": "2025-04-16T20:08:02.601Z"
      },
      {
        "duration": 1987,
        "start_time": "2025-04-16T20:08:16.912Z"
      },
      {
        "duration": 4,
        "start_time": "2025-04-16T20:08:18.905Z"
      },
      {
        "duration": 16,
        "start_time": "2025-04-16T20:08:21.456Z"
      },
      {
        "duration": 2051,
        "start_time": "2025-04-16T20:08:30.687Z"
      },
      {
        "duration": 7,
        "start_time": "2025-04-16T20:08:35.748Z"
      },
      {
        "duration": 11,
        "start_time": "2025-04-16T20:08:36.778Z"
      }
    ],
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "practicumNLP-py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
